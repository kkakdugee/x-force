{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the pipeline for querying and storing arxiv data into csv files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Usage / Strategy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEPRECATED:\n",
    "For future developers, important features about the arXiv API:\n",
    "- **it lacks a \"search by published date\" feature** (very problematic)\n",
    "- **in a query, the first return entry is always index 0** (meaning index 0 is different every query, also problematic)\n",
    "- it has the `\"start\"` & `\"max_results\"` paramaters that allow you to slice out a subsection of the entire query (starting from index `start`, return `max_result` entries)\n",
    "- it returns a max view of 30,000 but only allows you to retrieve a slice of 2000\n",
    "- it requires a 3 second wait between each query (or IP banned)\n",
    "\n",
    "I figured out a way to circumvent the lack of a \"search by published date\" feature below:\n",
    "- using the `\"sortOrder\": 'ascending'` parameter, we can **fix** the oldest entry as index 0\n",
    "- using `\"start\"` & `\"max_results\"` parameter, we can find out at which index the year 2020 papers started and then always consistently query from that point onwards\n",
    "- because there aren't 30,000 published papers in the queries this data pipeline was built upon, it is unclear how the indexing will change when the 30,000 limit has been surpassed\n",
    "    - this is a likely place to check if future bugs pop up\n",
    "\n",
    "Takeaways\n",
    "- this entire data pipeline is built on the `\"sortOrder\": 'ascending'`; please do not mess with this\n",
    "- (also, API documentation is lacking and debugging this seemingly super tiny issue took waaaay too long)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import feedparser\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables and Script Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../Modules/\")\n",
    "import helper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_request(query: str=helper.DEFAULT_SEARCH_QUERY, start: int=0, max_results: int=25) -> feedparser.util.FeedParserDict:\n",
    "    \"\"\"\n",
    "    Performs a fetch request using the arXiv API, returning the most recently published results first.\n",
    "\n",
    "    query -> str\n",
    "        The given search query for arxiv to find papers on\n",
    "\n",
    "    start -> int\n",
    "        The index of the papers at which to start pulling data on\n",
    "\n",
    "    max_results -> int\n",
    "        The total number of papers after `start` to pull from; cannot exceed the value stored in helper.ARXIV_FETCH_LIMIT\n",
    "\n",
    "    Returns -> feedparser.util.FeedParserDict\n",
    "        A feedparser.util.FeedParserDict object that contains the JSON parsed data\n",
    "    \n",
    "    Example\n",
    "        feed = fetch_request(query=search_term, 10, 50)\n",
    "        Query \"radiation\" returns 30,000 (default API behavior). For our slice, return the 11th article on the list (10th index) up to 49th article on the list, returning 50 total articles.\n",
    "    \"\"\"\n",
    "    if max_results > helper.ARXIV_FETCH_LIMIT:\n",
    "        raise ValueError(f\"Your query size is too large and will result in an IP ban. The limit is {helper.ARXIV_FETCH_LIMIT}.\")\n",
    "    else:\n",
    "        # Request setup\n",
    "        params = {\n",
    "            \"search_query\": query,\n",
    "            \"sortBy\": 'submittedDate',\n",
    "            \"sortOrder\": 'descending',\n",
    "            \"start\": start,\n",
    "            \"max_results\": max_results\n",
    "        }\n",
    "        response = requests.get(helper.DEFAULT_URL, params=params)\n",
    "\n",
    "        # Sleep to prevent rate limit\n",
    "        print(f\"Sleeping {helper.MIN_WAIT_TIME}\")\n",
    "        time.sleep(helper.MIN_WAIT_TIME)\n",
    "\n",
    "        # Return\n",
    "        if response.status_code == 200:\n",
    "            feed = feedparser.parse(response.content)\n",
    "            print(f\"Fetched {len(feed.entries)} entries.\")\n",
    "            return feed\n",
    "        else:\n",
    "            raise ConnectionError(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_request(feed: feedparser.util.FeedParserDict, query: str, verbose: int=1) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the given JSON feed file into a legible dataframe (useful for .csv storage).\n",
    "\n",
    "    feed -> feedparser.util.FeedParserDict\n",
    "        The given JSON object from feedparser\n",
    "\n",
    "    query -> str\n",
    "        The query term that was used to generate the feed. This is not enforced to be correct, so users need to manually double-check that this field is correct.\n",
    "        Used in .csv saving.\n",
    "\n",
    "    verbose -> int\n",
    "        0: suppresses all missing-data-points reporting\n",
    "        1: at the end of script, summarize the total number of missing data points\n",
    "        \n",
    "    Returns -> pd.core.frame.DataFrame\n",
    "        The JSON object converted to a dataframe\n",
    "\n",
    "    Example\n",
    "        search_term = \"radiation\"\n",
    "        feed = fetch_request(query=search_term, 10, 50)\n",
    "        df = parse_request(feed, query=search_term)\n",
    "    \"\"\"\n",
    "    # Parsing\n",
    "    all_papers = []\n",
    "    num_missing_keys = 0\n",
    "    for paper in feed.entries:\n",
    "        paper_data = [\"arxiv\", query, datetime.now()]\n",
    "        for key in helper.ARXIV_KEYS:\n",
    "            try:\n",
    "                if key == \"summary\":\n",
    "                    paper_data.append(paper[key].replace(\"\\n\", \" \"))\n",
    "                elif key == \"authors\":\n",
    "                    paper_data.append([item[\"name\"] for item in paper[key]])\n",
    "                elif key == \"link\":\n",
    "                    paper_data.append(paper[key][:-2])\n",
    "                else:\n",
    "                    paper_data.append(paper[key])\n",
    "            except:\n",
    "                paper_data.append(np.nan)\n",
    "                num_missing_keys += 1\n",
    "        all_papers.append(paper_data)\n",
    "    if verbose == 1:\n",
    "        print(f\"{num_missing_keys} missing keys.\")\n",
    "    df = pd.DataFrame(data=all_papers, columns=helper.MASTER_CSV_COLUMNS)\n",
    "    print(\"Parsed!\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_fetch_parse_request(query: str=helper.DEFAULT_SEARCH_QUERY, start: int=0, max_results: int=25, verbose: int=1) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Helper function for the main fetch_parse_request() function.\n",
    "\n",
    "    query -> str\n",
    "        The given search query for arxiv to find papers on\n",
    "\n",
    "    start -> int\n",
    "        The index of the papers at which to start pulling data on\n",
    "\n",
    "    max_results -> int\n",
    "        The total number of papers after `start` to pull from; cannot exceed the value stored in helper.ARXIV_FETCH_LIMIT\n",
    "\n",
    "    verbose -> int\n",
    "        0: suppresses all missing-data-points reporting\n",
    "        1: at the end of script, summarize the total number of missing data points\n",
    "        \n",
    "    Returns -> pd.core.frame.DataFrame\n",
    "        The fetch results converted as dataframe.\n",
    "    \"\"\"\n",
    "    feed = fetch_request(query=query, start=start, max_results=max_results)\n",
    "    df = parse_request(feed, query=query, verbose=verbose)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_remove_dupes(df: pd.core.frame.DataFrame, verbose: int=1) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes the duplicate entries (checked via URL base) from a given dataframe containing recently fetched queries.\n",
    "\n",
    "    df -> pd.core.frame.DataFrame\n",
    "        The given df object containing the parsed data from a query.\n",
    "    \n",
    "    verbose -> int\n",
    "        0: suppresses reporting on count of entry changes to df.\n",
    "        1: reports on count of entry changes to df.     \n",
    "\n",
    "    Returns -> pd.core.frame.DataFrame\n",
    "        A new df containing non-duplicate entries. Ready to be added to the complete_db.csv.\n",
    "\n",
    "    Example\n",
    "        unique_df = helper_remove_dupes(df)\n",
    "    \"\"\"\n",
    "    # Check verbose\n",
    "    if verbose not in {0,1}:\n",
    "        print(f\"Invalid verbose argument: {verbose}. Must be (0,1)\")\n",
    "        return None\n",
    "    \n",
    "    if verbose == 1:\n",
    "        pre_len = len(df)\n",
    "\n",
    "    # Removing dupes\n",
    "    database = pd.read_csv(\"../../data/complete_db.csv\")\n",
    "    database = database[database[\"source\"] == \"arxiv\"]\n",
    "    checks = database[\"url\"].values.tolist()\n",
    "    for check in checks:\n",
    "        df = df[df[\"url\"] != check]\n",
    "    \n",
    "    if verbose == 1:\n",
    "        post_len = len(df)\n",
    "        print(f\"Removed {pre_len - post_len} duplicates ({pre_len} -> {post_len}).\")\n",
    "\n",
    "    # Return\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_parse_request(query: str=helper.DEFAULT_SEARCH_QUERY, start: int=0, max_results: int=25, verbose: int=1, remove_dupes: int=1) -> list:\n",
    "    \"\"\"\n",
    "    Wrapper that combines both the fetching and parsing of a request. Handles requests larger than 2000. See individual functions for more details.\n",
    "\n",
    "    query -> str\n",
    "        The given search query for arxiv to find papers on\n",
    "\n",
    "    start -> int\n",
    "        The index of the papers at which to start pulling data on\n",
    "\n",
    "    max_results -> int\n",
    "        The total number of papers after `start` to pull from; cannot exceed the value stored in helper.ARXIV_FETCH_LIMIT\n",
    "\n",
    "    verbose -> int\n",
    "        0: suppresses all missing-data-points reporting\n",
    "        1: at the end of script, summarize the total number of missing data points\n",
    "    \n",
    "    remove_dupes -> int\n",
    "        0: does not remove duplicates as part of its fetch\n",
    "        1: removes duplicates as part of its fetch\n",
    "\n",
    "    Returns -> pd.core.frame.DataFrame\n",
    "        The fetch results converted as dataframe.\n",
    "    \"\"\"\n",
    "    # Check verbose\n",
    "    if verbose not in {0,1}:\n",
    "        raise ValueError(f\"Invalid verbose argument: {verbose}. Must be (0,1)\")\n",
    "    elif remove_dupes not in {0,1}:\n",
    "        raise ValueError(f\"Invalid remove_dupes argument: {remove_dupes}. Must be (0,1). See helper_remove_dupes().\")\n",
    "\n",
    "    # Check query size\n",
    "    count_of_results = max_results - start + 1\n",
    "    if count_of_results > helper.ARXIV_VIEW_LIMIT:\n",
    "        raise ValueError(f\"Invalid start and max_result options. You are fetching {count_of_results} results. Arxiv limits to {helper.ARXIV_VIEW_LIMIT}.\")\n",
    "\n",
    "    # Calculate loops\n",
    "    full_loops = max_results // helper.ARXIV_FETCH_LIMIT\n",
    "    partial_max_results = max_results - full_loops * helper.ARXIV_FETCH_LIMIT\n",
    "\n",
    "    # Fetch results\n",
    "    results = []\n",
    "    segment_cnt = 1\n",
    "    for i in range(0, full_loops*helper.MAX_STEP_SIZE, helper.MAX_STEP_SIZE):\n",
    "        print(f\"Fetch/parse segment {segment_cnt}/{full_loops+1}...\")\n",
    "        temp_df = helper_fetch_parse_request(query=query, start=i, max_results=helper.MAX_STEP_SIZE, verbose=verbose)\n",
    "        results.append(temp_df)\n",
    "        segment_cnt += 1\n",
    "    print(f\"Fetch/parse segment {segment_cnt}/{full_loops+1}...\")\n",
    "    temp_df = helper_fetch_parse_request(query=query, start=full_loops*helper.MAX_STEP_SIZE, max_results=partial_max_results, verbose=verbose)\n",
    "    results.append(temp_df)\n",
    "\n",
    "    if remove_dupes == 1:\n",
    "        results = [helper_remove_dupes(df) for df in results]\n",
    "    \n",
    "    # Return\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_request(list_of_dfs: list, verbose: int=1) -> None:\n",
    "    \"\"\"\n",
    "    Merges the dfs of paper entries into to the completed_db.csv, regardless of whether paper entries are duplicates.\n",
    "\n",
    "    list_of_dfs -> list\n",
    "        List of pd.core.frame.DataFrames to merge with completed_db.csv.\n",
    "    \n",
    "    verbose -> int\n",
    "        0: suppresses reporting on count of entry changes to database.\n",
    "        1: reports on count of entry changes to database. \n",
    "    \n",
    "    Returns -> None\n",
    "        complete_db.csv is updated with new entries.\n",
    "\n",
    "    Example\n",
    "        list_of_dfs = fetch_parse_request()\n",
    "        merge_request(list_of_dfs)\n",
    "    \"\"\"\n",
    "    # Check verbose\n",
    "    if verbose not in {0,1}:\n",
    "        print(f\"Invalid verbose argument: {verbose}. Must be (0,1)\")\n",
    "        return None\n",
    "\n",
    "    if verbose == 1:\n",
    "            database = pd.read_csv(\"../../data/complete_db.csv\")\n",
    "            super_pre_len = len(database)\n",
    "\n",
    "    for index, df in enumerate(list_of_dfs):\n",
    "        print(f\"Merging {index+1}/{len(list_of_dfs)}...\")\n",
    "        # Save df\n",
    "        if verbose == 1:\n",
    "            database = pd.read_csv(\"../../data/complete_db.csv\")\n",
    "            pre_len = len(database)\n",
    "            print(f\"Attempting to add {len(df)} entries...\")\n",
    "\n",
    "        try:\n",
    "            df.to_csv(\"../../data/complete_db.csv\", mode='a', index=False, header=False)\n",
    "            print(\"Saved!\")\n",
    "        except:\n",
    "            print(\"Failed to save.\")\n",
    "\n",
    "        if verbose == 1:\n",
    "            database = pd.read_csv(\"../../data/complete_db.csv\")\n",
    "            post_len = len(database)\n",
    "            print(f\"Added {post_len - pre_len} entries ({pre_len} -> {post_len})!\")\n",
    "    \n",
    "    if verbose == 1:\n",
    "        database = pd.read_csv(\"../../data/complete_db.csv\")\n",
    "        super_post_len = len(database)\n",
    "        print(f\"In summary, added {super_post_len - super_pre_len} entries ({super_pre_len} -> {super_post_len})!\")\n",
    "\n",
    "    # return\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_request(query: str=helper.DEFAULT_SEARCH_QUERY, start: int=0, max_results: int=25, verbose: int=1, remove_dupes: int=1) -> None:\n",
    "    \"\"\"\n",
    "    Adds indicated number of paper entries for the given query to the completed_db.csv.\n",
    "\n",
    "    query -> str\n",
    "        The given search query for arxiv to find papers on\n",
    "\n",
    "    start -> int\n",
    "        The index of the papers at which to start pulling data on\n",
    "\n",
    "    max_results -> int\n",
    "        The total number of papers after `start` to pull from\n",
    "\n",
    "    verbose -> int\n",
    "        0: suppresses reporting on changes to the database\n",
    "        1: reports on changes to database\n",
    "\n",
    "    remove_dupes -> int\n",
    "        0: does not remove duplicates as part of its fetch\n",
    "        1: removes duplicates as part of its fetch\n",
    "\n",
    "    Returns -> None\n",
    "        complete_db.csv is updated with new entries\n",
    "    \n",
    "    Example\n",
    "        pull_request(query=\"radiation\", max_results=15000, remove_dupes=1)\n",
    "    \"\"\"\n",
    "    print(\"------PART 1: FETCH PARSE\")\n",
    "    list_of_dfs = fetch_parse_request(query=query, start=start, max_results=max_results, verbose=verbose, remove_dupes=remove_dupes)\n",
    "    print(\"------PART 2: MERGE\")\n",
    "    merge_request(list_of_dfs, verbose=verbose)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_requests(queries: list, start: int=0, max_results: int=25, verbose: int=1, remove_dupes: int=1) -> None:\n",
    "    \"\"\"\n",
    "    Adds indicated number of paper entries for the given queries to the completed_db.csv.\n",
    "\n",
    "    queries -> list\n",
    "        The given search queries for arxiv to find papers on, contained in a list\n",
    "\n",
    "    start -> int\n",
    "        The index of the papers at which to start pulling data on (for each query)\n",
    "\n",
    "    max_results -> int\n",
    "        The total number of papers after `start` to pull from (for each query)\n",
    "\n",
    "    verbose -> int\n",
    "        0: suppresses reporting on changes to the database\n",
    "        1: reports on changes to database\n",
    "\n",
    "    remove_dupes -> int\n",
    "        0: does not remove duplicates as part of its fetch\n",
    "        1: removes duplicates as part of its fetch\n",
    "\n",
    "    Returns -> None\n",
    "        complete_db.csv is updated with new entries\n",
    "    \n",
    "    Example\n",
    "        queries = [\"radiation\", \"plasmonics\", \"stem cell\"]\n",
    "        pull_requests(queries, max_results=15000, remove_dupes=1)\n",
    "    \"\"\"\n",
    "    for query in queries:\n",
    "        print(\"--------------------\")\n",
    "        print(f\"PULLING {query}.\")\n",
    "        print(\"--------------------\")\n",
    "        pull_request(query=query, start=start, max_results=max_results, verbose=verbose, remove_dupes=remove_dupes)\n",
    "        print(f\"DONE WITH {query}.\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dupes(verbose: int=1) -> None:\n",
    "    \"\"\" \n",
    "    Manually remove duplicates in complete_db.csv of duplicate arvix entries.\n",
    "\n",
    "    verbose -> int\n",
    "        0: suppresses reporting on changes to the database\n",
    "        1: reports on changes to database\n",
    "    \n",
    "    Returns -> None\n",
    "        complete_db.csv is clean of duplicates via the \"url\" column\n",
    "    \n",
    "    Example\n",
    "        remove_dupes()\n",
    "    \"\"\"\n",
    "    database = pd.read_csv(\"../../data/complete_db.csv\")\n",
    "    database = database[database[\"source\"] == \"arxiv\"]\n",
    "    \n",
    "    if verbose == 1:\n",
    "        pre_len = len(database)\n",
    "    \n",
    "    database = database[~database.duplicated(\"url\")]\n",
    "    \n",
    "    if verbose == 1:\n",
    "        post_len = len(database)\n",
    "        print(f\"Removed {post_len - pre_len} duplicates ({pre_len} -> {post_len})!\")\n",
    "    \n",
    "    print(\"Completed.\")\n",
    "    return database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESET ARXIV DATABASES; CAREFUL WHEN RUNNING\n",
    "# helper.reset_papers_db(#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleeping 3\n",
      "Fetched 2000 entries.\n",
      "3055 missing keys.\n",
      "Parsed!\n",
      "Sleeping 3\n",
      "Fetched 2000 entries.\n",
      "2260 missing keys.\n",
      "Parsed!\n",
      "Sleeping 3\n",
      "Fetched 1000 entries.\n",
      "1055 missing keys.\n",
      "Parsed!\n",
      "Removed 31 duplicates (2000 -> 1969).\n",
      "Removed 0 duplicates (2000 -> 2000).\n",
      "Removed 0 duplicates (1000 -> 1000).\n",
      "Merging 1/3...\n",
      "Attempting to add 1969 entries...\n",
      "Saved!\n",
      "Added 1969 entries (73 -> 2042)!\n",
      "Merging 2/3...\n",
      "Attempting to add 2000 entries...\n",
      "Saved!\n",
      "Added 2000 entries (2042 -> 4042)!\n",
      "Merging 3/3...\n",
      "Attempting to add 1000 entries...\n",
      "Saved!\n",
      "Added 1000 entries (4042 -> 5042)!\n",
      "In summary, added 4969 entries (73 -> 5042)!\n",
      "--\n",
      "Sleeping 3\n",
      "Fetched 2000 entries.\n",
      "2178 missing keys.\n",
      "Parsed!\n",
      "Sleeping 3\n",
      "Fetched 2000 entries.\n",
      "1597 missing keys.\n",
      "Parsed!\n",
      "Sleeping 3\n",
      "Fetched 1000 entries.\n",
      "816 missing keys.\n",
      "Parsed!\n",
      "Removed 80 duplicates (2000 -> 1920).\n",
      "Removed 0 duplicates (2000 -> 2000).\n",
      "Removed 0 duplicates (1000 -> 1000).\n",
      "Merging 1/3...\n",
      "Attempting to add 1920 entries...\n",
      "Saved!\n",
      "Added 1920 entries (5042 -> 6962)!\n",
      "Merging 2/3...\n",
      "Attempting to add 2000 entries...\n",
      "Saved!\n",
      "Added 2000 entries (6962 -> 8962)!\n",
      "Merging 3/3...\n",
      "Attempting to add 1000 entries...\n",
      "Saved!\n",
      "Added 1000 entries (8962 -> 9962)!\n",
      "In summary, added 4920 entries (5042 -> 9962)!\n",
      "--\n",
      "Sleeping 3\n",
      "Fetched 2000 entries.\n",
      "2131 missing keys.\n",
      "Parsed!\n",
      "Sleeping 3\n",
      "Fetched 1798 entries.\n",
      "1413 missing keys.\n",
      "Parsed!\n",
      "Sleeping 3\n",
      "Fetched 0 entries.\n",
      "0 missing keys.\n",
      "Parsed!\n",
      "Removed 296 duplicates (2000 -> 1704).\n",
      "Removed 112 duplicates (1798 -> 1686).\n",
      "Removed 0 duplicates (0 -> 0).\n",
      "Merging 1/3...\n",
      "Attempting to add 1704 entries...\n",
      "Saved!\n",
      "Added 1704 entries (9962 -> 11666)!\n",
      "Merging 2/3...\n",
      "Attempting to add 1686 entries...\n",
      "Saved!\n",
      "Added 1686 entries (11666 -> 13352)!\n",
      "Merging 3/3...\n",
      "Attempting to add 0 entries...\n",
      "Saved!\n",
      "Added 0 entries (13352 -> 13352)!\n",
      "In summary, added 3390 entries (9962 -> 13352)!\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "queries = [\"radiation\", \"plasmonics\", \"metamaterials\"]\n",
    "pull_requests(queries, max_results=5000, remove_dupes=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DoD-XForce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
