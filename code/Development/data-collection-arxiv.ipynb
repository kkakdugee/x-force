{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the pipeline for querying and storing arxiv data into csv files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Usage / Strategy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEPRECATED:\n",
    "For future developers, important features about the arXiv API:\n",
    "- **it lacks a \"search by published date\" feature** (very problematic)\n",
    "- **in a query, the first return entry is always index 0** (meaning index 0 is different every query, also problematic)\n",
    "- it has the `\"start\"` & `\"max_results\"` paramaters that allow you to slice out a subsection of the entire query (starting from index `start`, return `max_result` entries)\n",
    "- it returns a max view of 30,000 but only allows you to retrieve a slice of 2000\n",
    "- it requires a 3 second wait between each query (or IP banned)\n",
    "\n",
    "I figured out a way to circumvent the lack of a \"search by published date\" feature below:\n",
    "- using the `\"sortOrder\": 'ascending'` parameter, we can **fix** the oldest entry as index 0\n",
    "- using `\"start\"` & `\"max_results\"` parameter, we can find out at which index the year 2020 papers started and then always consistently query from that point onwards\n",
    "- because there aren't 30,000 published papers in the queries this data pipeline was built upon, it is unclear how the indexing will change when the 30,000 limit has been surpassed\n",
    "    - this is a likely place to check if future bugs pop up\n",
    "\n",
    "Takeaways\n",
    "- this entire data pipeline is built on the `\"sortOrder\": 'ascending'`; please do not mess with this\n",
    "- (also, API documentation is lacking and debugging this seemingly super tiny issue took waaaay too long)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import feedparser\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables and Script Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"../Modules\")\n",
    "import helper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_request(url: str=helper.DEFAULT_URL, query: str=helper.DEFAULT_SEARCH_QUERY, start: int=0, max_results: int=10) -> feedparser.util.FeedParserDict:\n",
    "    \"\"\"\n",
    "    Performs a fetch request using the arXiv API on the given URL, returning the most recently published results first.\n",
    "\n",
    "    url -> str\n",
    "        The given url to send the fetch request to\n",
    "\n",
    "    query -> str\n",
    "        The given search query for arxiv to find papers on\n",
    "\n",
    "    start -> int\n",
    "        The index of the papers at which to start pulling data on\n",
    "\n",
    "    max_results -> int\n",
    "        The total number of papers after `start` to pull from; cannot exceed 2000\n",
    "\n",
    "    Returns -> feedparser.util.FeedParserDict\n",
    "        A feedparser.util.FeedParserDict object that contains the JSON parsed data\n",
    "    \n",
    "    Example\n",
    "        fetch_request(helper.DEFAULT_URL, \"radiation\", 10, 50)\n",
    "        Query \"radiation\" returns 30,000 (default API behavior). For our slice, return the 11th article on the list (10th index) up to 49th article on the list, returning 50 total articles.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"search_query\": query,\n",
    "        \"sortBy\": 'submittedDate',\n",
    "        \"sortOrder\": 'descending',\n",
    "        \"start\": start,\n",
    "        \"max_results\": max_results\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        feed = feedparser.parse(response.content)\n",
    "        print(f\"Fetched {len(feed.entries)} entries.\")\n",
    "        return feed\n",
    "    else:\n",
    "        raise ConnectionError(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_request(feed: feedparser.util.FeedParserDict, query: str, verbose: int=0) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the given JSON feed file into a legible dataframe (useful for .csv storage).\n",
    "\n",
    "    feed -> feedparser.util.FeedParserDict\n",
    "        The given JSON object from feedparser\n",
    "\n",
    "    query -> str\n",
    "        The query term that was used to generate the feed. This is not enforced to be correct, so users need to manually double-check that this field is correct.\n",
    "        Used in .csv saving.\n",
    "\n",
    "    verbose -> int\n",
    "        0: suppresses all missing-data-points reporting\n",
    "        1: at the end of script, summarize the total number of missing data points.\n",
    "        2: for every queried paper, error report on the missing data features; at the end of script, also summarize the total number of missing data points.\n",
    "        \n",
    "    Returns -> pd.core.frame.DataFrame\n",
    "        The JSON object converted to a dataframe\n",
    "\n",
    "    Example\n",
    "        parse_request(feed, \"radiation\", 0)\n",
    "    \"\"\"\n",
    "    all_papers = []\n",
    "    num_missing_keys = 0\n",
    "    for paper in feed.entries:\n",
    "        paper_data = [query, datetime.now()]\n",
    "        for key in helper.ARXIV_KEYS:\n",
    "            try:\n",
    "                if key == \"summary\":\n",
    "                    paper_data.append(paper[key].replace(\"\\n\", \" \"))\n",
    "                elif key == \"authors\":\n",
    "                    paper_data.append([item[\"name\"] for item in paper[key]])\n",
    "                else:\n",
    "                    paper_data.append(paper[key])\n",
    "            except:\n",
    "                paper_data.append(np.nan)\n",
    "                if verbose == 2:\n",
    "                    print(f\"{paper['id']} does not have {key} key\")\n",
    "                num_missing_keys += 1\n",
    "        all_papers.append(paper_data)\n",
    "    if verbose == 1 or verbose == 2:\n",
    "        print(f\"{num_missing_keys} missing keys.\")\n",
    "    df = pd.DataFrame(data=all_papers, columns=helper.MASTER_CSV_COLUMNS)\n",
    "    print(\"Parsed!\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_request_for_published(feed: feedparser.util.FeedParserDict, tar_date: str=helper.DEFAULT_DATE_QUERY) -> int:\n",
    "    \"\"\"\n",
    "    Finds the first index position of a paper whose published date is either on or after the given target date (useful for selective querying).\n",
    "\n",
    "    feed -> feedparser.util.FeedParserDict\n",
    "        The given JSON object from feedparser\n",
    "\n",
    "    tar_date -> str\n",
    "        The given target date\n",
    "        \n",
    "    Returns -> int, None\n",
    "        If found index, returns int\n",
    "        Else, return None\n",
    "\n",
    "    Example\n",
    "        crawl_request_for_published(feed, \"2010-2-31\")\n",
    "    \"\"\"\n",
    "    target_date_split = tar_date.split(\"-\")\n",
    "    if len(target_date_split) != 3:\n",
    "        raise ValueError(f\"ensure that your target date format is correct: year-month-day. eg. 2010-2-31\")\n",
    "    else:\n",
    "        search_target_date = datetime(int(target_date_split[0]), int(target_date_split[1]), int(target_date_split[2]))\n",
    "\n",
    "    for paper_index in range(len(feed.entries)):\n",
    "        raw_date_split = feed.entries[paper_index][\"published\"].split(\"T\")[0].split(\"-\")\n",
    "        paper_published_date = datetime(int(raw_date_split[0]), int(raw_date_split[1]), int(raw_date_split[2]))\n",
    "        if paper_published_date <= search_target_date:\n",
    "            print(f\"Found index for target date: {paper_index}!\")\n",
    "            return paper_index\n",
    "    print(\"Did not find index for target date!\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_parse_and_crawl(query: str=helper.DEFAULT_SEARCH_QUERY, tar_date: str=helper.DEFAULT_DATE_QUERY) -> tuple:\n",
    "    \"\"\"\n",
    "    Helper function that automatically saves all queried data to database and return the starting index.\n",
    "\n",
    "    query -> str\n",
    "        The given search query\n",
    "\n",
    "    tar_date -> str\n",
    "        The given target date\n",
    "        \n",
    "    Returns -> int, None\n",
    "        If found index, returns int\n",
    "        Else, return None\n",
    "\n",
    "    Example\n",
    "        helper_parse_and_crawl(\"radiation\", \"2010-2-31\")\n",
    "    \"\"\"\n",
    "    # every value in the up to 30,000 records that comes with the view\n",
    "    final_index = 0\n",
    "    for i in range(0, helper.MAX_VIEW_SIZE, helper.MAX_STEP_SIZE):\n",
    "        # parsing all data\n",
    "        print(\"--\")\n",
    "        print(f\"Checking index {i}-{i+helper.MAX_STEP_SIZE}\")\n",
    "        print(\"Fetching request...\")\n",
    "        feed = fetch_request(helper.DEFAULT_URL, query, i, helper.MAX_STEP_SIZE)\n",
    "        print(\"Parsing request...\")\n",
    "        df = parse_request(feed, query, 0)\n",
    "        print(\"Crawling request for target date...\")\n",
    "        index = crawl_request_for_published(feed, tar_date)\n",
    "\n",
    "        # Saving data to csv\n",
    "        print(f\"Saving {i}-{i+helper.MAX_STEP_SIZE}...\")\n",
    "        try:\n",
    "            df.to_csv(\"../../data/arxiv.csv\", mode='a', header=False, index=False)\n",
    "            print(\"Successfully saved!\")\n",
    "        except:\n",
    "            print(\"Failed to save.\")\n",
    "\n",
    "        # Returning index if found\n",
    "        if type(index) == int:\n",
    "            print(f\"Found target date! Sub-index: {index}, total: {final_index + index}\")\n",
    "            entry = df.iloc[index, :]\n",
    "            final_index += index\n",
    "            return (final_index, entry)\n",
    "        else:\n",
    "            final_index += helper.MAX_STEP_SIZE\n",
    "        \n",
    "        # Query wait\n",
    "        time.sleep(helper.MIN_WAIT_TIME) # wait or else arXiv will IP ban\n",
    "    print(f\"Did not find index for target date in entire window.\")\n",
    "    return (np.nan, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_papers_and_start_index(query: str=helper.DEFAULT_SEARCH_QUERY, tar_date: str=helper.DEFAULT_DATE_QUERY) -> None:\n",
    "    \"\"\"\n",
    "    Fetches papers of the query up to the tar_date; saves data in the meta-data database.\n",
    "\n",
    "    query -> str\n",
    "        The given search query \n",
    "\n",
    "    tar_date -> str\n",
    "        The given target date\n",
    "        \n",
    "    Returns\n",
    "        None\n",
    "\n",
    "    Example\n",
    "        get_papers_and_start_index(\"radiation\", \"2010-2-31\")\n",
    "    \"\"\"\n",
    "    final_index, entry = helper_parse_and_crawl(query, tar_date)\n",
    "    try:\n",
    "        data_constr = [query,\n",
    "                       entry[\"url\"],\n",
    "                       entry[\"title\"],\n",
    "                       entry[\"published\"],\n",
    "                       final_index,\n",
    "                       tar_date,\n",
    "                       datetime.now()\n",
    "                       ]\n",
    "    except:\n",
    "        data_constr = [query,\n",
    "                       np.nan,\n",
    "                       np.nan,\n",
    "                       np.nan,\n",
    "                       np.nan,\n",
    "                       tar_date,\n",
    "                       datetime.now()\n",
    "                       ]\n",
    "    \n",
    "    # Saving \n",
    "    df = pd.DataFrame(data=np.array(data_constr).reshape(1, 7), columns=helper.META_CSV_COLUMNS)\n",
    "    try:\n",
    "        df.to_csv(\"../../data/arxiv_meta_data.csv\", mode='a', index=False, header=False)\n",
    "        print(\"Saved!\")\n",
    "    except:\n",
    "        print(\"Failed to save...\")\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_meta_papers_db():\n",
    "    \"\"\"\n",
    "    Overwrites the existing metadata database.\n",
    "    USE WITH CAUTION.\n",
    "\n",
    "    Returns\n",
    "        None\n",
    "\n",
    "    Example\n",
    "        reset_meta_papers_db()\n",
    "    \"\"\"\n",
    "    # Warning menu\n",
    "    while True:\n",
    "        user_input = input(\"Are you sure you want to run this function? This wipes the ENTIRE existing arxiv_meta_data.csv database! (y/n)\")\n",
    "        if user_input == \"y\":\n",
    "            print(\"Proceeding to reset database.\")\n",
    "            break\n",
    "        elif user_input == \"n\":\n",
    "            print(\"Function canceled.\")\n",
    "            return None\n",
    "        else:\n",
    "            print(\"Wrong input. Please type 'y' or 'n'.\")\n",
    "\n",
    "    # Saving data to csv\n",
    "    df = pd.DataFrame(columns=helper.META_CSV_COLUMNS)\n",
    "    try:\n",
    "        df.to_csv(\"../../data/arxiv_meta_data.csv\", index=False)\n",
    "        print(\"Saved!\")\n",
    "    except:\n",
    "        print(\"Failed to save...\")\n",
    "\n",
    "    # Return\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_papers_db() -> None:\n",
    "    \"\"\"\n",
    "    Resets/overwrites the \"arxiv.csv\" database.\n",
    "    USE WITH CAUTION.\n",
    "\n",
    "    Returns\n",
    "        None\n",
    "\n",
    "    Example\n",
    "        reset_papers_db()\n",
    "    \"\"\"\n",
    "    # Warning menu\n",
    "    while True:\n",
    "        user_input = input(\"Are you sure you want to run this function? This wipes the ENTIRE existing arxiv.csv database! (y/n)\")\n",
    "        if user_input == \"y\":\n",
    "            print(\"Proceeding to reset database.\")\n",
    "            break\n",
    "        elif user_input == \"n\":\n",
    "            print(\"Function canceled.\")\n",
    "            return None\n",
    "        else:\n",
    "            print(\"Wrong input. Please type 'y' or 'n'.\")\n",
    "\n",
    "    # Saving data to csv\n",
    "    df = pd.DataFrame(columns=helper.MASTER_CSV_COLUMNS)\n",
    "    try:\n",
    "        df.to_csv(\"../../data/arxiv.csv\", index=False)\n",
    "        print(\"Saved!\")\n",
    "    except:\n",
    "        print(\"Failed to save...\")\n",
    "\n",
    "    # Return\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESET ARXIV DATABASES; CAREFUL WHEN RUNNING\n",
    "# reset_meta_papers_db#()\n",
    "# reset_papers_db#()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_papers_and_start_index(\"radiation\", helper.DEFAULT_DATE_QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15758"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "15758\n",
    "\"http://arxiv.org/abs/1001.0353v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 100 entries.\n",
      "Parsed!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>query_time</th>\n",
       "      <th>title</th>\n",
       "      <th>journal</th>\n",
       "      <th>authors</th>\n",
       "      <th>doi</th>\n",
       "      <th>published</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>2023-06-27 15:51:14.133004</td>\n",
       "      <td>Orion's Bar: Physical Conditions across the De...</td>\n",
       "      <td>Astrophys.J.693:285-302,2009</td>\n",
       "      <td>[E. W. Pellegrini, J. A. Baldwin, G. J. Ferlan...</td>\n",
       "      <td>10.1088/0004-637X/693/1/285</td>\n",
       "      <td>2008-11-07T16:46:17Z</td>\n",
       "      <td>Previous work has shown the Orion Bar to be an...</td>\n",
       "      <td>http://arxiv.org/abs/0811.1176v2</td>\n",
       "      <td>[{'term': 'astro-ph', 'scheme': 'http://arxiv....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>2023-06-27 15:51:14.133004</td>\n",
       "      <td>Stochastic self-similarity of envelopes of hig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[A. A. Gusev]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-11-07T16:48:09Z</td>\n",
       "      <td>High-frequency (HF) seismic radiation of large...</td>\n",
       "      <td>http://arxiv.org/abs/0811.1177v1</td>\n",
       "      <td>[{'term': 'physics.geo-ph', 'scheme': 'http://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>2023-06-27 15:51:14.133004</td>\n",
       "      <td>The Wouthuysen-Field effect in a clumpy interg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Jonathan Higgins, Avery Meiksin]</td>\n",
       "      <td>10.1111/j.1365-2966.2008.14199.x</td>\n",
       "      <td>2008-11-07T17:23:00Z</td>\n",
       "      <td>We show that, due to the high optical depth of...</td>\n",
       "      <td>http://arxiv.org/abs/0811.1184v1</td>\n",
       "      <td>[{'term': 'astro-ph', 'scheme': 'http://arxiv....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>2023-06-27 15:51:14.133004</td>\n",
       "      <td>Gas accretion onto planetary cores: three-dime...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Ben A. Ayliffe, Matthew R. Bate]</td>\n",
       "      <td>10.1111/j.1365-2966.2008.14184.x</td>\n",
       "      <td>2008-11-08T09:50:38Z</td>\n",
       "      <td>We present results from three-dimensional, sel...</td>\n",
       "      <td>http://arxiv.org/abs/0811.1259v2</td>\n",
       "      <td>[{'term': 'astro-ph', 'scheme': 'http://arxiv....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>2023-06-27 15:51:14.133004</td>\n",
       "      <td>Cosmic Strings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[A. Achucarro, C. J. A. P. Martins]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-11-08T17:10:01Z</td>\n",
       "      <td>Cosmic strings are linear concentrations of en...</td>\n",
       "      <td>http://arxiv.org/abs/0811.1277v1</td>\n",
       "      <td>[{'term': 'astro-ph', 'scheme': 'http://arxiv....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>test</td>\n",
       "      <td>2023-06-27 15:51:14.134030</td>\n",
       "      <td>A new Concept of Ball Lightning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Levan N. Tsintsadze]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-11-28T02:39:04Z</td>\n",
       "      <td>We suggest that the ball lightning (BL) is a w...</td>\n",
       "      <td>http://arxiv.org/abs/0811.4640v1</td>\n",
       "      <td>[{'term': 'physics.ao-ph', 'scheme': 'http://a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>test</td>\n",
       "      <td>2023-06-27 15:51:14.134030</td>\n",
       "      <td>Direct Simulations of Particle Acceleration in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Takayuki Muranushi, Shu-ichiro Inutsuka]</td>\n",
       "      <td>10.1088/0004-637X/691/1/L24</td>\n",
       "      <td>2008-11-28T11:29:18Z</td>\n",
       "      <td>We simulate the acceleration processes of coll...</td>\n",
       "      <td>http://arxiv.org/abs/0811.4528v1</td>\n",
       "      <td>[{'term': 'astro-ph', 'scheme': 'http://arxiv....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>test</td>\n",
       "      <td>2023-06-27 15:51:14.134030</td>\n",
       "      <td>On the non-Gaussianity from Recombination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Nicola Bartolo, Antonio Riotto]</td>\n",
       "      <td>10.1088/1475-7516/2009/03/017</td>\n",
       "      <td>2008-11-28T15:27:40Z</td>\n",
       "      <td>The non-linear effects operating at the recomb...</td>\n",
       "      <td>http://arxiv.org/abs/0811.4584v3</td>\n",
       "      <td>[{'term': 'astro-ph', 'scheme': 'http://arxiv....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>test</td>\n",
       "      <td>2023-06-27 15:51:14.134030</td>\n",
       "      <td>2D non-LTE radiative modelling of He I spectra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[L. Leger, F. Paletou]</td>\n",
       "      <td>10.1051/0004-6361/200810296</td>\n",
       "      <td>2008-11-28T17:12:42Z</td>\n",
       "      <td>The diagnosis of new high-resolution spectropo...</td>\n",
       "      <td>http://arxiv.org/abs/0811.4753v1</td>\n",
       "      <td>[{'term': 'astro-ph', 'scheme': 'http://arxiv....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>test</td>\n",
       "      <td>2023-06-27 15:51:14.134030</td>\n",
       "      <td>A Turbulent Model of Gamma-Ray Burst Variability</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Ramesh Narayan, Pawan Kumar]</td>\n",
       "      <td>10.1111/j.1745-3933.2009.00624.x</td>\n",
       "      <td>2008-11-28T22:06:01Z</td>\n",
       "      <td>A popular paradigm to explain the rapid tempor...</td>\n",
       "      <td>http://arxiv.org/abs/0812.0018v1</td>\n",
       "      <td>[{'term': 'astro-ph', 'scheme': 'http://arxiv....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   query                 query_time  \\\n",
       "0   test 2023-06-27 15:51:14.133004   \n",
       "1   test 2023-06-27 15:51:14.133004   \n",
       "2   test 2023-06-27 15:51:14.133004   \n",
       "3   test 2023-06-27 15:51:14.133004   \n",
       "4   test 2023-06-27 15:51:14.133004   \n",
       "..   ...                        ...   \n",
       "95  test 2023-06-27 15:51:14.134030   \n",
       "96  test 2023-06-27 15:51:14.134030   \n",
       "97  test 2023-06-27 15:51:14.134030   \n",
       "98  test 2023-06-27 15:51:14.134030   \n",
       "99  test 2023-06-27 15:51:14.134030   \n",
       "\n",
       "                                                title  \\\n",
       "0   Orion's Bar: Physical Conditions across the De...   \n",
       "1   Stochastic self-similarity of envelopes of hig...   \n",
       "2   The Wouthuysen-Field effect in a clumpy interg...   \n",
       "3   Gas accretion onto planetary cores: three-dime...   \n",
       "4                                      Cosmic Strings   \n",
       "..                                                ...   \n",
       "95                    A new Concept of Ball Lightning   \n",
       "96  Direct Simulations of Particle Acceleration in...   \n",
       "97          On the non-Gaussianity from Recombination   \n",
       "98  2D non-LTE radiative modelling of He I spectra...   \n",
       "99   A Turbulent Model of Gamma-Ray Burst Variability   \n",
       "\n",
       "                         journal  \\\n",
       "0   Astrophys.J.693:285-302,2009   \n",
       "1                            NaN   \n",
       "2                            NaN   \n",
       "3                            NaN   \n",
       "4                            NaN   \n",
       "..                           ...   \n",
       "95                           NaN   \n",
       "96                           NaN   \n",
       "97                           NaN   \n",
       "98                           NaN   \n",
       "99                           NaN   \n",
       "\n",
       "                                              authors  \\\n",
       "0   [E. W. Pellegrini, J. A. Baldwin, G. J. Ferlan...   \n",
       "1                                       [A. A. Gusev]   \n",
       "2                   [Jonathan Higgins, Avery Meiksin]   \n",
       "3                   [Ben A. Ayliffe, Matthew R. Bate]   \n",
       "4                 [A. Achucarro, C. J. A. P. Martins]   \n",
       "..                                                ...   \n",
       "95                              [Levan N. Tsintsadze]   \n",
       "96          [Takayuki Muranushi, Shu-ichiro Inutsuka]   \n",
       "97                   [Nicola Bartolo, Antonio Riotto]   \n",
       "98                             [L. Leger, F. Paletou]   \n",
       "99                      [Ramesh Narayan, Pawan Kumar]   \n",
       "\n",
       "                                 doi             published  \\\n",
       "0        10.1088/0004-637X/693/1/285  2008-11-07T16:46:17Z   \n",
       "1                                NaN  2008-11-07T16:48:09Z   \n",
       "2   10.1111/j.1365-2966.2008.14199.x  2008-11-07T17:23:00Z   \n",
       "3   10.1111/j.1365-2966.2008.14184.x  2008-11-08T09:50:38Z   \n",
       "4                                NaN  2008-11-08T17:10:01Z   \n",
       "..                               ...                   ...   \n",
       "95                               NaN  2008-11-28T02:39:04Z   \n",
       "96       10.1088/0004-637X/691/1/L24  2008-11-28T11:29:18Z   \n",
       "97     10.1088/1475-7516/2009/03/017  2008-11-28T15:27:40Z   \n",
       "98       10.1051/0004-6361/200810296  2008-11-28T17:12:42Z   \n",
       "99  10.1111/j.1745-3933.2009.00624.x  2008-11-28T22:06:01Z   \n",
       "\n",
       "                                             abstract  \\\n",
       "0   Previous work has shown the Orion Bar to be an...   \n",
       "1   High-frequency (HF) seismic radiation of large...   \n",
       "2   We show that, due to the high optical depth of...   \n",
       "3   We present results from three-dimensional, sel...   \n",
       "4   Cosmic strings are linear concentrations of en...   \n",
       "..                                                ...   \n",
       "95  We suggest that the ball lightning (BL) is a w...   \n",
       "96  We simulate the acceleration processes of coll...   \n",
       "97  The non-linear effects operating at the recomb...   \n",
       "98  The diagnosis of new high-resolution spectropo...   \n",
       "99  A popular paradigm to explain the rapid tempor...   \n",
       "\n",
       "                                 url  \\\n",
       "0   http://arxiv.org/abs/0811.1176v2   \n",
       "1   http://arxiv.org/abs/0811.1177v1   \n",
       "2   http://arxiv.org/abs/0811.1184v1   \n",
       "3   http://arxiv.org/abs/0811.1259v2   \n",
       "4   http://arxiv.org/abs/0811.1277v1   \n",
       "..                               ...   \n",
       "95  http://arxiv.org/abs/0811.4640v1   \n",
       "96  http://arxiv.org/abs/0811.4528v1   \n",
       "97  http://arxiv.org/abs/0811.4584v3   \n",
       "98  http://arxiv.org/abs/0811.4753v1   \n",
       "99  http://arxiv.org/abs/0812.0018v1   \n",
       "\n",
       "                                                 tags  \n",
       "0   [{'term': 'astro-ph', 'scheme': 'http://arxiv....  \n",
       "1   [{'term': 'physics.geo-ph', 'scheme': 'http://...  \n",
       "2   [{'term': 'astro-ph', 'scheme': 'http://arxiv....  \n",
       "3   [{'term': 'astro-ph', 'scheme': 'http://arxiv....  \n",
       "4   [{'term': 'astro-ph', 'scheme': 'http://arxiv....  \n",
       "..                                                ...  \n",
       "95  [{'term': 'physics.ao-ph', 'scheme': 'http://a...  \n",
       "96  [{'term': 'astro-ph', 'scheme': 'http://arxiv....  \n",
       "97  [{'term': 'astro-ph', 'scheme': 'http://arxiv....  \n",
       "98  [{'term': 'astro-ph', 'scheme': 'http://arxiv....  \n",
       "99  [{'term': 'astro-ph', 'scheme': 'http://arxiv....  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed = fetch_request(query=helper.DEFAULT_SEARCH_QUERY, start=15758, max_results=100)\n",
    "df = parse_request(feed, \"test\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>query_time</th>\n",
       "      <th>title</th>\n",
       "      <th>journal</th>\n",
       "      <th>authors</th>\n",
       "      <th>doi</th>\n",
       "      <th>published</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [query, query_time, title, journal, authors, doi, published, abstract, url, tags]\n",
       "Index: []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"url\"] == \"http://arxiv.org/abs/1001.03536\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNDER Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4042111915.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    test =\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_fetch_papers(query: str=helper.DEFAULT_SEARCH_QUERY, verbose:int=0) -> None:\n",
    "    \"\"\"\n",
    "    Resets/overwrites the \"arxiv.csv\" database with the 5 most recently published papers about the given query.\n",
    "\n",
    "    query -> str\n",
    "        The given query/search term\n",
    "\n",
    "    verbose -> int\n",
    "        0: suppresses all missing-data-points reporting\n",
    "        1: at the end of script, summarize the total number of missing data points.\n",
    "        2: for every queried paper, error report on the missing data features; at the end of script, also summarize the total number of missing data points.\n",
    "        \n",
    "    Returns\n",
    "        None\n",
    "\n",
    "    Example\n",
    "        fetch_initial_papers(\"radiation\", 0)\n",
    "    \"\"\"\n",
    "    # Error control\n",
    "    valid_verbose = {0, 1, 2}\n",
    "    if verbose not in valid_verbose:\n",
    "        raise ValueError(f\"verbose input must be one of the following: {valid_verbose}\")\n",
    "\n",
    "    # Set request params\n",
    "    params = {\n",
    "        \"search_query\": query,\n",
    "        \"sortBy\": 'submittedDate', # do not change this\n",
    "        \"sortOrder\": 'ascending', # do not change this\n",
    "        \"start\": 0,\n",
    "        \"max_results\": 5\n",
    "    }\n",
    "\n",
    "    # Fetching request\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        feed = feedparser.parse(response.content)\n",
    "        print(f\"Fetched {len(feed.entries)} entries.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Parsing request\n",
    "    all_papers = []\n",
    "    num_missing_keys = 0\n",
    "    for paper in feed.entries:\n",
    "        paper_data = []\n",
    "        for key in ARXIV_KEYS:\n",
    "            try:\n",
    "                paper_data.append(paper[key])\n",
    "            except:\n",
    "                paper_data.append(np.nan)\n",
    "                if verbose == 2:\n",
    "                    print(f\"{paper['id']} does not have {key} key\")\n",
    "                num_missing_keys += 1\n",
    "        all_papers.append(paper_data)\n",
    "    if verbose == 1 or verbose == 2:\n",
    "        print(f\"{num_missing_keys} missing keys.\")\n",
    "\n",
    "    # Saving data to csv\n",
    "    df = pd.DataFrame(data=all_papers, columns=helper.MASTER_CSV_COLUMNS)\n",
    "    try:\n",
    "        df.to_csv(\"../../data/arxiv.csv\", index=False)\n",
    "        print(\"Saved!\")\n",
    "    except:\n",
    "        print(\"Failed to save...\")\n",
    "\n",
    "    # Return\n",
    "    return None\n",
    "\n",
    "def fetch_more_papers(query: str=helper.DEFAULT_SEARCH_QUERY, verbose: int=0, n: int=50) -> None:\n",
    "    \"\"\"\n",
    "    TODO: make this docstring better lol\n",
    "    fetches the next n papers published after the oldest entry in the csv\n",
    "    \"\"\"\n",
    "    # Extract oldest published date in dataset \n",
    "    df = pd.read_csv(\"../../data/arxiv.csv\")\n",
    "    start_date = df[\"published\"].iloc[-1:].values[0]\n",
    "\n",
    "    # Set request params\n",
    "    params = {\n",
    "        \"search_query\": query,\n",
    "        \"start_date\": start_date,\n",
    "        \"sortBy\": 'submittedDate',  # relevance, lastUpdatedDate, submittedDate\n",
    "        \"max_results\": n,\n",
    "        \"sortOrder\": 'descending'\n",
    "    }\n",
    "\n",
    "    # Fetching request\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        feed = feedparser.parse(response.content)\n",
    "        print(f\"Fetched {len(feed.entries)} entries.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Parsing request\n",
    "    all_papers = []\n",
    "    num_missing_keys = 0\n",
    "    for paper in feed.entries:\n",
    "        paper_data = []\n",
    "        for key in ARXIV_KEYS:\n",
    "            try:\n",
    "                paper_data.append(paper[key])\n",
    "            except:\n",
    "                paper_data.append(np.nan)\n",
    "                if verbose == 1:\n",
    "                    print(f\"{paper['id']} does not have {key} key\")\n",
    "                    num_missing_keys += 1\n",
    "                else:\n",
    "                    num_missing_keys += 1\n",
    "        all_papers.append(paper_data)\n",
    "    print(f\"{num_missing_keys} missing keys.\")\n",
    "\n",
    "    # Saving data to csv\n",
    "    df = pd.DataFrame(data=all_papers, columns=MASTER_CSV_COLUMNS)\n",
    "    try:\n",
    "        df.to_csv(\"../../data/arxiv.csv\", mode=\"a\", index=False, header=False)\n",
    "        print(\"Saved!\")\n",
    "    except:\n",
    "        print(\"Failed to save...\")\n",
    "\n",
    "    # Return\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fetch_more_papers()\n",
      "Cell \u001b[1;32mIn[12], line 89\u001b[0m, in \u001b[0;36mfetch_more_papers\u001b[1;34m(query, verbose, n)\u001b[0m\n\u001b[0;32m     80\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[0;32m     81\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msearch_query\u001b[39m\u001b[39m\"\u001b[39m: query,\n\u001b[0;32m     82\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstart_date\u001b[39m\u001b[39m\"\u001b[39m: start_date,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msortOrder\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mdescending\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     86\u001b[0m }\n\u001b[0;32m     88\u001b[0m \u001b[39m# Fetching request\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url, params\u001b[39m=\u001b[39mparams)\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m     91\u001b[0m     feed \u001b[39m=\u001b[39m feedparser\u001b[39m.\u001b[39mparse(response\u001b[39m.\u001b[39mcontent)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'url' is not defined"
     ]
    }
   ],
   "source": [
    "fetch_more_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_papers(query: str = DEFAULT_SEARCH_QUERY, verbose:int = 0, n:int = 50) -> None:\n",
    "    \"\"\" \n",
    "    TODO docstring\n",
    "\n",
    "    wrapper for fetch more papers\n",
    "    \"\"\"\n",
    "    if n <= 0:\n",
    "        print(\"n cannot be negative or 0.\")\n",
    "        return None\n",
    "    else:\n",
    "        mult_of_10 = n // 10\n",
    "        leftoever_of_10 = n - mult_of_10\n",
    "\n",
    "        if mult_of_10 > 0:\n",
    "            for _ in range(mult_of_10):\n",
    "                fetch_more_papers(query, verbose)\n",
    "        fetch_more_papers(query, verbose, leftoever_of_10)\n",
    "        print(\"Finished loops!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 10 entries.\n",
      "16 missing keys.\n",
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "# WARNING, RUNNING THIS FUNCTION WILL RESET THE DATABASE\n",
    "fetch_initial_papers(DEFAULT_SEARCH_QUERY, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1000 entries.\n",
      "1698 missing keys.\n",
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "fetch_more_papers(DEFAULT_SEARCH_QUERY, 0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DoD-XForce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
