{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "sys.path.append(\"../Modules/\")\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNorm(helper.Normalize):\n",
    "    \"\"\"Map a list of text values to the float range 0-1\"\"\"\n",
    "\n",
    "    def __init__(self, textvals, clip=False):\n",
    "        self._clip = clip\n",
    "        # if you want, clean text here, for duplicate, sorting, etc\n",
    "        ltextvals = set(textvals)\n",
    "        self.N = len(ltextvals)\n",
    "        self.textmap = dict(\n",
    "            [(text, float(i)/(self.N-1)) for i, text in enumerate(ltextvals)])\n",
    "        self._vmin = 0\n",
    "        self._vmax = 1\n",
    "\n",
    "    def __call__(self, x, clip=None):\n",
    "        ret = helper.ma.asarray([self.textmap.get(xkey, -1) for xkey in x])\n",
    "        return ret\n",
    "\n",
    "    def inverse(self, value):\n",
    "        return ValueError(\"TextNorm is not invertible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_demo = True\n",
    "class XForce_Grapher():\n",
    "    def __init__(self) -> None:\n",
    "        self._data = None\n",
    "        self._sources = None\n",
    "        self._queries = None\n",
    "        self._summary = None\n",
    "        self._nlp_summary = None\n",
    "        self._data_size = None\n",
    "        self._sparse_matrix = None\n",
    "        self._sparse_matrix_names = None\n",
    "        if is_demo:\n",
    "            self.load(\"../../data/demo_db.csv\")\n",
    "        else:\n",
    "            self.load(\"../../data/complete_db.csv\")\n",
    "        return None\n",
    "\n",
    "    def load(self, path: str) -> None:\n",
    "        self.load_db(path)\n",
    "        self.load_db_summary()\n",
    "        return None\n",
    "\n",
    "    def load_db(self, path: str) -> None:\n",
    "        \"\"\" \n",
    "        # TODO DOCUMENTATION\n",
    "        \"\"\"\n",
    "        df = helper.pd.read_csv(path)\n",
    "        sources = list(set(df[\"source\"].values.tolist()))\n",
    "        sources.append(\"ALL\")\n",
    "        queries = list(set(df[\"query\"].values.tolist()))\n",
    "        queries.append(\"ALL\")\n",
    "\n",
    "        self._data = df\n",
    "        self._data_size = len(df)\n",
    "        self._sources = sources\n",
    "        self._queries = queries\n",
    "        return None\n",
    "\n",
    "    def load_db_summary(self) -> None:\n",
    "        \"\"\"\n",
    "        Generates the report table of the count of paper entries by query and by source and stores it in class attribute.\n",
    "\n",
    "        Returns -> None\n",
    "            Stores the report dataframe into class attribute.\n",
    "\n",
    "        Example\n",
    "            grapher = XForce_Grapher()\n",
    "            grapher.load_db_summary()\n",
    "        \"\"\"\n",
    "        df = self._data.copy()\n",
    "        sources = self._sources.copy()\n",
    "        queries = self._queries.copy()\n",
    "        l = [[\"source\"], queries]\n",
    "        data_header = [item for sublist in l for item in sublist]\n",
    "        data_rows = []\n",
    "        for source in sources:\n",
    "            data_row = [source]\n",
    "            if source == \"ALL\":\n",
    "                filtered_by_source_df = df\n",
    "            else:\n",
    "                filtered_by_source_df = df[df[\"source\"] == source]\n",
    "            for query in queries:\n",
    "                filtered_by_query_df = filtered_by_source_df[filtered_by_source_df[\"query\"] == query]\n",
    "                data_row.append(len(filtered_by_query_df))\n",
    "            data_rows.append(data_row)\n",
    "        report = helper.pd.DataFrame(data=data_rows, columns=data_header).iloc[:, :-1]\n",
    "        self._summary = report\n",
    "        return None\n",
    "\n",
    "    def peek_data(self):\n",
    "        return self._data.head()\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self._data\n",
    "\n",
    "    def get_sources(self):\n",
    "        return self._sources\n",
    "\n",
    "    def get_queries(self):\n",
    "        return self._queries\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return self._summary\n",
    "\n",
    "    def peek_nlp_summary(self):\n",
    "        if self._nlp_summary is not None:\n",
    "            return self._nlp_summary.sort_values(by=\"abstract_word_count\").reset_index().iloc[helper.np.r_[0:5, -5:0]]\n",
    "        else:\n",
    "            print(\"Please run .load_nlp_summary() first.\")\n",
    "            return None\n",
    "\n",
    "    def get_nlp_summary(self):\n",
    "        if self._nlp_summary is not None:\n",
    "            return self._nlp_summary\n",
    "        else:\n",
    "            print(\"Please run .load_nlp_summary() first.\")\n",
    "            return None\n",
    "\n",
    "    def set_data(self, path: str) -> None:\n",
    "        self.load(path)\n",
    "        return None\n",
    "\n",
    "    def graph_pub_freq(self, \n",
    "                       queries: list=[\"ALL\"], \n",
    "                       sources: list=[\"ALL\"],\n",
    "                       country_mode: int=1) -> None:\n",
    "        \"\"\"\n",
    "        # TODO Fix scopus date. Do not you run \"ALL\" for sources.\n",
    "        # TODO Add country_mode\n",
    "\n",
    "        Graphs the publishing frequency of papers in the database.\n",
    "\n",
    "        queries -> list\n",
    "            The given list of search queries (can be a 1-item list) that match database queries\n",
    "            ALL: Considers all queries\n",
    "\n",
    "        sources -> list\n",
    "            The given list of search sources (can be a 1-item list) that match database sources\n",
    "            ALL: Considers all sources\n",
    "\n",
    "        country_mode -> int\n",
    "            0: graphs the bars of the bar chart as normal bars\n",
    "            1: graphs the bars of the bar chart as stacked bar charts corresponding to country of publishing institution\n",
    "\n",
    "        Returns -> None\n",
    "            Shows matplotlib graph of the published frequency\n",
    "        \n",
    "        Example\n",
    "            grapher = XForce_Grapher()\n",
    "            grapher.graph_pub_freq([\"ALL\"], [\"ALL\"], 1)\n",
    "        \"\"\"\n",
    "        # Input Error Handling\n",
    "        country_mode_options = {0,1}\n",
    "        if country_mode not in country_mode_options:\n",
    "            print(f\"{country_mode} invalid, must be {country_mode_options}\")\n",
    "            return None\n",
    "\n",
    "        for source in sources:\n",
    "            if source not in self._sources:\n",
    "                raise ValueError(f\"{source} invalid, must be {self._sources}\")\n",
    "        \n",
    "        for query in queries:\n",
    "            if query not in self._queries:\n",
    "                raise ValueError(f\"{query} invalid, must be {self._queries}\")\n",
    "\n",
    "        # Load Data\n",
    "        df = self._data.copy()\n",
    "        \n",
    "        # Filter Data\n",
    "        title_sources, title_queries = [\"ALL\"], [\"ALL\"]\n",
    "        if \"ALL\" not in sources:\n",
    "            expression = helper.generate_boolean_conditions(\"source\", sources)\n",
    "            df = df[eval(expression)]\n",
    "            title_sources = sources\n",
    "        if \"ALL\" not in queries:\n",
    "            expression = helper.generate_boolean_conditions(\"query\", queries)\n",
    "            df = df[eval(expression)]\n",
    "            title_queries = queries\n",
    "        \n",
    "        # Data Setup\n",
    "        dates_extract = df[\"published\"].apply(lambda x: x.split('T')[0])\n",
    "        dates = [helper.datetime(int(i.split(\"-\")[0]), int(i.split(\"-\")[1]), int(i.split(\"-\")[2])) for i in dates_extract]\n",
    "        \n",
    "        # Graph\n",
    "        helper.plt.title(f\"Source: {title_sources}, Query: {title_queries}\", fontsize=3)\n",
    "        helper.plt.suptitle(f\"Publish Frequency within {len(dates)} Most Recent Papers\")\n",
    "        helper.plt.xlabel(\"Publish Dates\")\n",
    "        helper.plt.ylabel(\"Frequency\")\n",
    "        helper.plt.grid(\"True\")\n",
    "        helper.plt.xticks(rotation=45)\n",
    "        helper.plt.hist(dates, 25, alpha=.75)\n",
    "        helper.plt.tight_layout()\n",
    "        helper.plt.savefig(f\"../images/pub_freq/pub_freq_{'_'.join(title_sources)}_{'_'.join(title_queries)}.png\")\n",
    "        helper.plt.show()\n",
    "\n",
    "        # Return\n",
    "        return None\n",
    "    \n",
    "    def report_db_summary(self) -> None:\n",
    "        \"\"\" \n",
    "        Prints a report table of the count of paper entries by query and by source.\n",
    "\n",
    "        Returns -> None\n",
    "            Both prints and returns the report dataframe.\n",
    "\n",
    "        Example\n",
    "            grapher = XForce_Grapher()\n",
    "            grapher.report_db_summary()\n",
    "        \"\"\"\n",
    "        print(self._summary)\n",
    "        return None\n",
    "\n",
    "    def graph_db_summary(self) -> None:\n",
    "        \"\"\"\n",
    "        Graphs the report summary as a stacked barchart.\n",
    "\n",
    "        Returns -> None\n",
    "            Prints out matplotlib graph of the report summary\n",
    "        \n",
    "        Example\n",
    "            grapher = XForce_Grapher()\n",
    "            grapher.graph_db_summary()\n",
    "        \"\"\"\n",
    "        df = self._summary.copy()\n",
    "        extract_counts = df.T.values.tolist()[1:-1] # -1 to remove the \"ALL\" from the category\n",
    "        extract_queries = df.T.index.tolist()[1:-1]\n",
    "        sources = df[\"source\"]\n",
    "\n",
    "        query_count_data = {}\n",
    "        for i in range(len(extract_counts)):\n",
    "            query_count_data[extract_queries[i]] = extract_counts[i]\n",
    "        \n",
    "        width = 0.5\n",
    "        fig, ax = helper.plt.subplots()\n",
    "        bottom = helper.np.zeros(3)\n",
    "\n",
    "        for query, count in query_count_data.items():\n",
    "            p = ax.bar(sources, count, width, label=query, bottom=bottom)\n",
    "            bottom += count\n",
    "\n",
    "        helper.plt.title(f\"Distribution of {self._data_size} Articles\")\n",
    "        helper.plt.xlabel(\"Source\")\n",
    "        helper.plt.ylabel(\"Counts\")\n",
    "        helper.plt.grid(\"True\")\n",
    "        helper.plt.legend(loc='upper left', bbox_to_anchor=(1,1))\n",
    "        helper.plt.tight_layout()\n",
    "        helper.plt.savefig(f\"../images/db_summ/db_summ.png\")\n",
    "        helper.plt.show()\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def helper_remove_stopwords(self, input):\n",
    "        \"\"\"\n",
    "        # TODO fill in docstring\n",
    "        \"\"\"\n",
    "        words = input.split(\" \")\n",
    "        filtered_words = [word for word in words if word not in helper.MASTER_STOP_WORDS]\n",
    "        output = \" \".join(filtered_words)\n",
    "        return output\n",
    "    \n",
    "    def helper_lemmatizer(self, input):\n",
    "        \"\"\"\n",
    "        # TODO fill in docstring\n",
    "        \"\"\"\n",
    "        wordnet_lemmatizer = helper.WordNetLemmatizer()\n",
    "        words = input.split(\" \")\n",
    "        lemma_words = [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
    "        output = \" \".join(lemma_words)\n",
    "        return output\n",
    "    \n",
    "    def helper_keyword_extractor(self, input):\n",
    "        \"\"\"\n",
    "        # TODO fill in docstring\n",
    "        \"\"\"\n",
    "        kw_model = helper.KeyBERT()\n",
    "        output = kw_model.extract_keywords(input, keyphrase_ngram_range=(1, 2), stop_words=helper.MASTER_STOP_WORDS)\n",
    "        return output\n",
    "\n",
    "    def helper_singularizer(self, input):\n",
    "        \"\"\"\n",
    "        # TODO fill in docstring\n",
    "        \"\"\"\n",
    "        blob = helper.TextBlob(input)\n",
    "        singular_nouns = [word.singularize() for word, tag in blob.tags if tag.startswith('NN')]\n",
    "        output = \" \".join(singular_nouns)\n",
    "        return output\n",
    "\n",
    "    def load_nlp_summary(self) -> None:\n",
    "        \"\"\"\n",
    "        Creates, pre-processes, and saves the NLP summary report in class variable, which is ready for analytics.\n",
    "\n",
    "        Returns -> None\n",
    "            Saves the NLP summary report.\n",
    "        \n",
    "        Example\n",
    "            grapher = XForce_Grapher()\n",
    "            grapher.load_nlp_summary()\n",
    "        \"\"\"\n",
    "        if is_demo:\n",
    "            print(\"Dynamic load disabled in demo mode due to 11hr+ runtime. All functions are ran on database snapshot.\")\n",
    "            self._nlp_summary = helper.pd.read_csv(\"../../data/demo_db.csv\")\n",
    "            return\n",
    "\n",
    "        # Filtering\n",
    "        df = self._data.copy()\n",
    "        df = df[[\"source\", \"query\", \"published\", \"url\", \"title\", \"abstract\"]]\n",
    "        \n",
    "        # Cleaning & Feature Creation\n",
    "        df.dropna(inplace=True)\n",
    "        col_to_process = [\"title\", \"abstract\"]\n",
    "        for col in col_to_process:\n",
    "            df[f\"{col}_char_count\"] = df.loc[:, f\"{col}\"].map(lambda x: len(x))\n",
    "            df[f\"{col}_word_count\"] = df.loc[:, f\"{col}\"].map(lambda x: len(x.split(\" \")))\n",
    "            print(f\"Finished {col} counter; moving to rules-preprocessing.\")\n",
    "            df.loc[:, col] = df.loc[:, col].apply(lambda x: x.lower())\n",
    "            df.loc[:, col] = df.loc[:, col].apply(lambda x: self.helper_remove_stopwords(x))\n",
    "            df.loc[:, col] = df.loc[:, col].apply(lambda x: helper.re.sub('[%s]' % helper.re.escape(helper.string.punctuation), ' ' , x))\n",
    "            df.loc[:, col] = df.loc[:, col].apply(lambda x: helper.re.sub(r'\\d+', '', x))\n",
    "            df.loc[:, col] = df.loc[:, col].apply(lambda x: helper.re.sub(' +', ' ', x))\n",
    "            df.loc[:, col] = df.loc[:, col].apply(lambda x: self.helper_lemmatizer(x))\n",
    "            print(f\"Finished {col} rules-preprocessing; moving to singularization-preprocessing.\")\n",
    "            df.loc[:, col] = df.loc[:, col].apply(lambda x: self.helper_singularizer(x))\n",
    "            print(f\"Finished {col} singularization-preprocessing; moving to embedded-preprocessing.\")\n",
    "            df[f\"{col}_keywords\"] = df.loc[:, f\"{col}\"].map(lambda x: self.helper_keyword_extractor(x))\n",
    "            print(f\"Finished {col} embedded-preprocessing.\")\n",
    "            \n",
    "        # Saving\n",
    "        self._nlp_summary = df\n",
    "        \n",
    "        # Return\n",
    "        return None\n",
    "    \n",
    "    def graph_text_freq(self, \n",
    "                         queries: list=[\"ALL\"], \n",
    "                         sources: list=[\"ALL\"], \n",
    "                         text_mode: str=\"word\", \n",
    "                         type_mode: str=\"abstract\") -> None:\n",
    "        \"\"\"\n",
    "        Graphs the text frequency (eg. character/word count of title/abstract) of indicated papers\n",
    "\n",
    "        queries -> list\n",
    "            The given list of search queries (can be a 1-item list) that match database queries\n",
    "            ALL: Considers all queries\n",
    "\n",
    "        sources -> list\n",
    "            The given list of search sources (can be a 1-item list) that match database sources\n",
    "            ALL: Considers all sources\n",
    "\n",
    "        text_mode -> str\n",
    "            Given type mode to filter on\n",
    "            \"char\": Graphs via character count\n",
    "            \"word\": Graphs via word count\n",
    "\n",
    "        type_mode -> str\n",
    "            Given type mode to filter on\n",
    "            \"title\": Graphs on title\n",
    "            \"abstract\": Graphs on abstract\n",
    "\n",
    "        Returns -> None\n",
    "            Shows matplotlib graph of the text counts\n",
    "        \n",
    "        Example\n",
    "            grapher = XForce_Grapher()\n",
    "            grapher.graph_text_freq(queries=[\"radiation\", \"plasmonics\"], source=[\"arxiv\"], text_mode=\"word\", type_mode=\"abstract\")\n",
    "        \"\"\"\n",
    "        # Input Error Handling\n",
    "        for source in sources:\n",
    "            if source not in self._sources:\n",
    "                raise ValueError(f\"{source} invalid, must be {self._sources}\")\n",
    "        \n",
    "        for query in queries:\n",
    "            if query not in self._queries:\n",
    "                raise ValueError(f\"{query} invalid, must be {self._queries}\")\n",
    "\n",
    "        text_mode_options = {\"char\", \"word\"}\n",
    "        if text_mode not in text_mode_options:\n",
    "            raise ValueError(f\"{text_mode} invalid; must be {text_mode_options}\")\n",
    "\n",
    "        type_mode_options = {\"title\", \"abstract\"}\n",
    "        if type_mode not in type_mode_options:\n",
    "            raise ValueError(f\"{type_mode} invalid; must be {type_mode_options}\")\n",
    "\n",
    "        # Load Data\n",
    "        if self._nlp_summary is None:\n",
    "            print(\"Please run .load_nlp_summary() first, ETA ~1 minute.\")\n",
    "            return\n",
    "        df = self._nlp_summary.copy()\n",
    "        \n",
    "        # Filter Data\n",
    "        title_sources, title_queries = [\"ALL\"], [\"ALL\"]\n",
    "        if \"ALL\" not in sources:\n",
    "            expression = helper.generate_boolean_conditions(\"source\", sources)\n",
    "            df = df[eval(expression)]\n",
    "            title_sources = sources\n",
    "        if \"ALL\" not in queries:\n",
    "            expression = helper.generate_boolean_conditions(\"query\", queries)\n",
    "            df = df[eval(expression)]\n",
    "            title_queries = queries\n",
    "        else:\n",
    "            queries = self._queries[:-1]\n",
    "\n",
    "        # Data Setup\n",
    "        expression = f\"{type_mode}_{text_mode}_count\"\n",
    "        graph_data = []\n",
    "        for query in queries:\n",
    "            graph_data.append(df[df[\"query\"] == query][expression])\n",
    "\n",
    "        # Graphing\n",
    "        helper.plt.title(f\"Source: {title_sources}, Query: {title_queries}\")\n",
    "        helper.plt.suptitle(f\"Summary Statistics of {type_mode.title()} {text_mode.title()} Count by Specified Papers\")\n",
    "        helper.plt.xlabel(\"Query\")\n",
    "        helper.plt.ylabel(\"Count\")\n",
    "        helper.plt.grid(\"True\")\n",
    "        helper.plt.xticks(rotation=45)\n",
    "        helper.plt.boxplot(graph_data, positions=helper.np.array(range(len(graph_data)))*2.0, sym='', widths=1.5)\n",
    "        helper.plt.xticks(range(0, len(queries)*2, 2), queries, rotation=45)\n",
    "        helper.plt.tight_layout()\n",
    "        helper.plt.savefig(f\"../images/text_freq/text_freq_{'_'.join(title_sources)}_{'_'.join(title_queries)}.png\")\n",
    "        helper.plt.show()\n",
    "\n",
    "        return None\n",
    "\n",
    "    def helper_extract_text(self, input):\n",
    "        input = eval(input)\n",
    "        output = [i[0] for i in input]\n",
    "        return output\n",
    "    \n",
    "    def helper_extract_value(self, input):\n",
    "        input = eval(input)\n",
    "        output = [i[1] for i in input]\n",
    "        return output\n",
    "    \n",
    "    def helper_extract_set(self, input):\n",
    "        output = set()\n",
    "        for i in input:\n",
    "            tokens = i.split(\" \")\n",
    "            for token in tokens:\n",
    "                output.add(token)\n",
    "        return list(output)\n",
    "\n",
    "    def graph_keyword_freq(self, \n",
    "                           queries: list=[\"ALL\"], \n",
    "                           sources: list=[\"ALL\"], \n",
    "                           type_mode: str=\"abstract\", \n",
    "                           k: int=15, \n",
    "                           n_gram: int=2) -> None:\n",
    "        \"\"\"\n",
    "        Graphs the keyword frequency of the specified papers. \n",
    "\n",
    "        queries -> list\n",
    "            The given list of search queries (can be a 1-item list) that match database queries\n",
    "            ALL: Considers all queries\n",
    "\n",
    "        sources -> list\n",
    "            The given list of search sources (can be a 1-item list) that match database sources\n",
    "            ALL: Considers all sources\n",
    "\n",
    "        type_mode -> str\n",
    "            Given type mode to filter on\n",
    "            \"title\": Graphs on title\n",
    "            \"abstract\": Graphs on abstract\n",
    "\n",
    "        k -> int\n",
    "            The number of top n-grams to be graphed\n",
    "\n",
    "        n_gram -> int\n",
    "            The n-grams to graph on\n",
    "\n",
    "        Returns -> None\n",
    "            Shows matplotlib graph of the text counts\n",
    "        \n",
    "        Example\n",
    "            grapher = XForce_Grapher()\n",
    "            grapher.graph_text_count(queries=[\"radiation\", \"plasmonics\"], source=\"arxiv\", type_mode=\"abstract\", k=15, n_grams=1)\n",
    "        \"\"\"\n",
    "        # Input Error Handling\n",
    "        for source in sources:\n",
    "            if source not in self._sources:\n",
    "                raise ValueError(f\"{source} invalid, must be {self._sources}\")\n",
    "        \n",
    "        for query in queries:\n",
    "            if query not in self._queries:\n",
    "                raise ValueError(f\"{query} invalid, must be {self._queries}\")\n",
    "\n",
    "        type_mode_options = {\"title\", \"abstract\"}\n",
    "        if type_mode not in type_mode_options:\n",
    "            raise ValueError(f\"{type_mode} invalid; must be {type_mode_options}\")\n",
    "\n",
    "        # Load Data\n",
    "        if self._nlp_summary is None:\n",
    "            print(\"Please run .load_nlp_summary() first, ETA ~1 minute.\")\n",
    "            return\n",
    "        df = self._nlp_summary.copy()\n",
    "        print(\"Running .graph_keyword_freq(), ETA ~1.5 minutes.\")\n",
    "\n",
    "        # Filter Data\n",
    "        title_sources, title_queries = [\"ALL\"], [\"ALL\"]\n",
    "        if \"ALL\" not in sources:\n",
    "            expression = helper.generate_boolean_conditions(\"source\", sources)\n",
    "            df = df[eval(expression)]\n",
    "            title_sources = sources\n",
    "        if \"ALL\" not in queries:\n",
    "            expression = helper.generate_boolean_conditions(\"query\", queries)\n",
    "            df = df[eval(expression)]\n",
    "            title_queries = queries\n",
    "        else:\n",
    "            queries = self._queries[:-1]\n",
    "        keywords = df.loc[:, f\"{type_mode}_keywords\"]\n",
    "        keyword_freq = {}\n",
    "        for list_of_keyword_tuples in keywords:\n",
    "            for keyword_tuple in list_of_keyword_tuples:\n",
    "                if len(keyword_tuple[0].split(\" \")) == n_gram:\n",
    "                    if keyword_tuple[0] in keyword_freq.keys():\n",
    "                        keyword_freq[keyword_tuple[0]] += 1\n",
    "                    else:\n",
    "                        keyword_freq[keyword_tuple[0]] = 1\n",
    "        keyword_list = [(k, v) for k, v in keyword_freq.items()]\n",
    "        keyword_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        keyword_names = [i[0] for i in keyword_list[:k]]\n",
    "        keyword_counts = [i[1] for i in keyword_list[:k]]\n",
    "        \n",
    "        # X = df.loc[:, type_mode] OLD METHOD\n",
    "        # NLP Cleaning OLD\n",
    "        # cvec = helper.CountVectorizer(ngram_range=(n_gram, n_gram))\n",
    "        # sparse = cvec.fit_transform(X)\n",
    "        # self._sparse_matrix = sparse\n",
    "        # names = cvec.get_feature_names_out()\n",
    "        # self._sparse_matrix_names = names\n",
    "        # ngram_counts = sparse.sum(axis=0).A1\n",
    "\n",
    "        # Data Setup OLD\n",
    "        # df = helper.pd.DataFrame(data=[ngram_counts], index=[\"counts\"], columns=names)\n",
    "        # df = df.sort_values(by=df.index[0], axis=1, ascending=False)\n",
    "\n",
    "        # Graphing\n",
    "        helper.plt.title(f\"Source: {title_sources}, Query: {title_queries}\")\n",
    "        helper.plt.suptitle(f\"Top {k} Most Common {n_gram}-Grams in Papers from Given Queries\")\n",
    "        helper.plt.xlabel(\"Count\")\n",
    "        helper.plt.ylabel(\"Tokens\")\n",
    "        helper.plt.grid(True)\n",
    "        helper.plt.barh(keyword_names[::-1], keyword_counts[::-1], alpha=0.5)\n",
    "\n",
    "        # Saving\n",
    "        helper.plt.tight_layout()\n",
    "        helper.plt.savefig(f\"../../images/keyword_freq/keyword_freq_{'_'.join(title_sources)}_{'_'.join(title_queries)}.png\")\n",
    "        helper.plt.show()\n",
    "\n",
    "        return None\n",
    "\n",
    "    def helper_eval(self, input):\n",
    "        output = eval(input)\n",
    "        return output\n",
    "\n",
    "    def graph_network_cooccurence(self, n: int=50, annotation_threshold: float=0.05) -> None:\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        # n = number of top keywords per query\n",
    "        # annotation_threshold = 0-1, any nodes of frequency count larger than threshold will be labeled\n",
    "        \"\"\"\n",
    "        # -----------------------\n",
    "        # DATA SETUP GRAPHING\n",
    "        # -----------------------\n",
    "        # Raw data extraction\n",
    "        corpus = self._nlp_summary[[\"abstract_keywords\", \"query\"]]\n",
    "        corpus[\"keyword_labels\"] = corpus.iloc[:, 0].apply(lambda x: self.helper_extract_text(x))\n",
    "        corpus[\"keyword_unique_labels\"] = corpus.iloc[:, 2].apply(lambda x: self.helper_extract_set(x))\n",
    "        corpus[\"keyword_meaningfulness\"] = corpus.iloc[:, 0].apply(lambda x: self.helper_extract_value(x))\n",
    "\n",
    "        # Edgelist data extraction\n",
    "        if is_demo:\n",
    "            df_edgelist = helper.pd.read_csv(\"../../data/demo_edgelist.csv\")\n",
    "            df_edgelist[\"0\"] = df_edgelist[\"0\"].apply(lambda x: helper_eval(x))\n",
    "            edgelist = list(zip(df_edgelist[\"0\"].values, df_edgelist[\"1\"].values))\n",
    "        else:\n",
    "            edgelist = []\n",
    "            unique_keys = set()\n",
    "            for set_item in corpus[\"keyword_unique_labels\"]:\n",
    "                for i in set_item:\n",
    "                    unique_keys.add(i)\n",
    "            for unique_key in unique_keys:\n",
    "                for row_index in range(len(corpus[\"keyword_unique_labels\"])):\n",
    "                    for term in corpus[\"keyword_unique_labels\"][row_index]:\n",
    "                        if unique_key == term:\n",
    "                            for i in range(len(corpus[\"keyword_unique_labels\"][row_index])):\n",
    "                                if corpus[\"keyword_unique_labels\"][row_index][i] != unique_key:\n",
    "                                    ordered_edge = sorted((unique_key, corpus[\"keyword_unique_labels\"][row_index][i]))\n",
    "                                    complete_edge = (ordered_edge, corpus[\"query\"][row_index])\n",
    "                                    edgelist.append(complete_edge)\n",
    "        \n",
    "        # Individual edgelist data stream extraction\n",
    "        freq_dict = {}\n",
    "        for cooccurence in edgelist:\n",
    "            stringify = \"-\".join(cooccurence[0])\n",
    "            if stringify in freq_dict.keys():\n",
    "                freq_dict[stringify][0] += 1\n",
    "            else:\n",
    "                freq_dict[stringify] = [1, cooccurence[1]]\n",
    "        summ_freq_edges = sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        unique_queries = self._queries.copy()\n",
    "        filtered_entries = []\n",
    "        for unique_query in unique_queries:\n",
    "            counter = 0\n",
    "            for tuple_entry in summ_freq_edges:\n",
    "                if counter < n:\n",
    "                    if tuple_entry[1][1] == unique_query:\n",
    "                        filtered_entries.append(tuple_entry)\n",
    "                        counter += 1\n",
    "\n",
    "        nodify, counts, queries = [], [], []\n",
    "        for item in filtered_entries:\n",
    "            node1, node2 = item[0].split(\"-\")\n",
    "            res_tuple = (node1, node2)\n",
    "            nodify.append(res_tuple)\n",
    "            counts.append(item[1][0])\n",
    "            queries.append(item[1][1])\n",
    "\n",
    "        # -----------------------\n",
    "        # NETWORK GRAPHING\n",
    "        # -----------------------\n",
    "        # Grapher setup\n",
    "        G = helper.nx.Graph()\n",
    "        G.add_edges_from(nodify)\n",
    "\n",
    "        # Calculate node sizes based on frequencies\n",
    "        node_sizes = {}\n",
    "        for node in G.nodes:\n",
    "            total_frequency = 0\n",
    "            for (u, v), frequency in zip(nodify, counts):\n",
    "                if node in (u, v):\n",
    "                    total_frequency += frequency\n",
    "            node_sizes[node] = total_frequency\n",
    "        label_threshold = max(node_sizes.values()) * annotation_threshold\n",
    "\n",
    "        # Constructing unique colors / category mappying\n",
    "        unique_queries = self._queries\n",
    "        color_map = helper.plt.cm.get_cmap('tab20', len(unique_queries))\n",
    "        node_to_category = dict(zip(G.nodes, queries))\n",
    "\n",
    "        # Figure setup\n",
    "        helper.plt.figure(figsize=(15, 15))\n",
    "        pos = helper.nx.kamada_kawai_layout(G)\n",
    "        helper.plt.axis('off')\n",
    "        helper.plt.title(f\"Network Co-Occurrence of Top {n} Keyterms in Each of {len(unique_queries)-1} Queries\")\n",
    "\n",
    "        # Plot nodes\n",
    "        node_size_list = [node_sizes[node] for node in G.nodes]\n",
    "        node_color_list = [color_map(unique_queries.index(node_to_category[node])) for node in G.nodes]\n",
    "        helper.nx.draw_networkx_nodes(G, pos, node_size=node_size_list, node_color=node_color_list, alpha=0.7)\n",
    "        helper.nx.draw_networkx_edges(G, pos, edge_color='gray', alpha=0.5)\n",
    "\n",
    "        # Annotate nodes\n",
    "        label_nodes = [node for node, size in node_sizes.items() if size > label_threshold]\n",
    "        labels = {node: node for node in label_nodes}\n",
    "        helper.nx.draw_networkx_labels(G, pos, labels, font_size=10, font_color='black')\n",
    "\n",
    "        # Legend\n",
    "        legend_entries = []\n",
    "        for category in unique_queries:\n",
    "            legend_entries.append(helper.plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map(unique_queries.index(category)), label=category))\n",
    "        helper.plt.legend(handles=legend_entries, title='Categories', title_fontsize=12)\n",
    "\n",
    "        # Display the plot\n",
    "        helper.plt.tight_layout()\n",
    "        helper.plt.savefig(f\"../../images/network_cooccur/network_cooccur_{n}.png\")\n",
    "        helper.plt.show()\n",
    "    \n",
    "    def graph_bubble_map(self, n: int=5, annotate_threshold: float=0.15) -> None:\n",
    "        \"\"\" \n",
    "        # TODO\n",
    "        # n = number of top keywords per query\n",
    "        # annotate_threshold = radius of individual circle, if radius is larger than given, then annotate\n",
    "        \"\"\"\n",
    "        # Data Cleaning\n",
    "        corpus = helper.pd.read_csv(\"../../data/demo_db.csv\")[[\"abstract_keywords\", \"query\"]]\n",
    "\n",
    "        # Data Cleaning\n",
    "        corpus[\"keyword_labels\"] = corpus.iloc[:, 0].apply(lambda x: self.helper_extract_text(x))\n",
    "        corpus[\"keyword_meaningfulness\"] = corpus.iloc[:, 0].apply(lambda x: self.helper_extract_value(x))\n",
    "\n",
    "        keyword_labels_ls = corpus[\"keyword_labels\"].values.tolist()\n",
    "        flat_list = [item for sublist in keyword_labels_ls for item in sublist]\n",
    "\n",
    "        word_freq = {}\n",
    "        for k, v in zip(flat_list, corpus[\"query\"]):\n",
    "            if k in word_freq.keys():\n",
    "                word_freq[k][0] += 1\n",
    "            else:\n",
    "                word_freq[k] = [1, v]\n",
    "        word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        unique_queries = self._queries.copy()\n",
    "        extracted_ls = []\n",
    "        for query in unique_queries:\n",
    "            counter = 0\n",
    "            for item in word_freq:\n",
    "                if counter < n:\n",
    "                    if item[1][1] == query:\n",
    "                        extracted_ls.append(item)\n",
    "                        counter += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        keys, values, queries = [], [], []\n",
    "        for item in extracted_ls:\n",
    "            keys.append(item[0])\n",
    "            values.append(item[1][0])\n",
    "            queries.append(item[1][1])\n",
    "\n",
    "        # Creating graphing circles\n",
    "        circles = helper.circlify.circlify(\n",
    "            values,\n",
    "            show_enclosure=True,\n",
    "            target_enclosure=helper.circlify.Circle(x=0, y=0, r=1)\n",
    "        )\n",
    "\n",
    "        # Setting up figures\n",
    "        fig, ax = helper.plt.subplots(figsize=(10,10))\n",
    "        ax.set_title(f\"Bubble Map of {n} Most Common Keyterms in Each of {len(unique_queries)-1} Queries\")\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Calculate boundaries of graph\n",
    "        lim = max(\n",
    "            max(\n",
    "                abs(circle.x) + circle.r,\n",
    "                abs(circle.y) + circle.r,\n",
    "            )\n",
    "            for circle in circles\n",
    "        )\n",
    "        helper.plt.xlim(-lim, lim)\n",
    "        helper.plt.ylim(-lim, lim)\n",
    "\n",
    "        # Setting up color/label scheme\n",
    "        norm = TextNorm(queries)\n",
    "        cmap = helper.cm.get_cmap('tab20')\n",
    "        label_color_dict = {}\n",
    "\n",
    "        # Plotting\n",
    "        for circle, annotation, label in zip(circles, keys, queries):\n",
    "            # Setting up color/label scheme\n",
    "            normalized_value = norm([label])[0]\n",
    "            if label not in label_color_dict:\n",
    "                color = cmap(normalized_value)\n",
    "                color = helper.mcolors.to_rgba(color)[:3]\n",
    "                label_color_dict[label] = color\n",
    "            color = label_color_dict[label]\n",
    "\n",
    "            # Graphing Circles\n",
    "            x, y, r = circle\n",
    "            ax.add_patch(helper.plt.Circle((x, y), r, alpha=0.9, linewidth=2, facecolor=color, label=label))\n",
    "            if r > annotate_threshold:\n",
    "                helper.plt.annotate(annotation, (x, y), va='center', ha='center', bbox=dict(facecolor='white', edgecolor='black', boxstyle='round', pad=.5))\n",
    "\n",
    "        # Legend\n",
    "        legend_patches = [helper.plt.Circle((0, 0), 0, facecolor=color, label=label) for label, color in label_color_dict.items()]\n",
    "        ax.legend(handles=legend_patches, labels=label_color_dict.keys(), bbox_to_anchor=(1, 1), loc='upper right')\n",
    "\n",
    "        # Show & Save\n",
    "        helper.plt.tight_layout()\n",
    "        helper.plt.savefig(\"../images/pub_freq/pub_freq_ALL_ALL.png\")\n",
    "        helper.plt.show()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic load disabled in demo mode due to 11hr+ runtime. All functions are ran on database snapshot.\n"
     ]
    }
   ],
   "source": [
    "test = XForce_Grapher()\n",
    "test.load_nlp_summary()\n",
    "df = test.get_nlp_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'big_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m helper\u001b[39m.\u001b[39mplt\u001b[39m.\u001b[39mtitle(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mQuery: \u001b[39m\u001b[39m{\u001b[39;00mdf[\u001b[39m'\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m helper\u001b[39m.\u001b[39mplt\u001b[39m.\u001b[39mgrid(\u001b[39m\"\u001b[39m\u001b[39mTrue\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m values \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(big_counts[\u001b[39m0\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m helper\u001b[39m.\u001b[39mplt\u001b[39m.\u001b[39mpie(values, labels\u001b[39m=\u001b[39mcountries)\n\u001b[0;32m      7\u001b[0m helper\u001b[39m.\u001b[39mplt\u001b[39m.\u001b[39mlegend(loc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mupper left\u001b[39m\u001b[39m'\u001b[39m, bbox_to_anchor\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'big_counts' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "countries = [\"US\", \"UK\", \"China\", \"Germany\", \"Switzerland\", \"Other\"]\n",
    "helper.plt.suptitle(f\"Distribution of Country Publishing\")\n",
    "helper.plt.title(f\"Query: {df['query'][0]}\")\n",
    "helper.plt.grid(\"True\")\n",
    "values = sorted(big_counts[0], reverse=True)\n",
    "helper.plt.pie(values, labels=countries)\n",
    "helper.plt.legend(loc='upper left', bbox_to_anchor=(1,1))\n",
    "helper.plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DoD-XForce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
