{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the pipeline for querying and storing arxiv data into csv files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Usage / Strategy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future developers, important features about the arXiv API:\n",
    "- **it lacks a \"search by published date\" feature** (very problematic)\n",
    "- **in a query, the first return entry is always index 0** (meaning index 0 is different every query, also problematic)\n",
    "- it has the `\"start\"` & `\"max_results\"` paramaters that allow you to slice out a subsection of the entire query (starting from index `start`, return `max_result` entries)\n",
    "- it returns a max view of 30,000 but only allows you to retrieve a slice of 2000\n",
    "- it requires a 3 second wait between each query (or IP banned)\n",
    "\n",
    "I figured out a way to circumvent the lack of a \"search by published date\" feature below:\n",
    "- using the `\"sortOrder\": 'ascending'` parameter, we can **fix** the oldest entry as index 0\n",
    "- using `\"start\"` & `\"max_results\"` parameter, we can find out at which index the year 2020 papers started and then always consistently query from that point onwards\n",
    "- because there aren't 30,000 published papers in the queries this data pipeline was built upon, it is unclear how the indexing will change when the 30,000 limit has been surpassed\n",
    "    - this is a likely place to check if future bugs pop up\n",
    "\n",
    "Takeaways\n",
    "- this entire data pipeline is built on the `\"sortOrder\": 'ascending'`; please do not mess with this\n",
    "- (also, API documentation is lacking and debugging this seemingly super tiny issue took waaaay too long)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import feedparser\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables and Script Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'helper' has no attribute 'DEFAULT_URL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch_request\u001b[39m(url: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m helper\u001b[39m.\u001b[39mDEFAULT_URL, params: \u001b[39mdict\u001b[39m \u001b[39m=\u001b[39m helper\u001b[39m.\u001b[39mDEFAULT_PARAMS) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m feedparser\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mFeedParserDict:\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m    Performs a fetch request using the arXiv API on the given URL/\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m        See `helper.py` file for the formats of the default values\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url, params\u001b[39m=\u001b[39mparams)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'helper' has no attribute 'DEFAULT_URL'"
     ]
    }
   ],
   "source": [
    "def fetch_request(url: str = helper.DEFAULT_URL, params: dict = helper.DEFAULT_PARAMS) -> feedparser.util.FeedParserDict:\n",
    "    \"\"\"\n",
    "    Performs a fetch request using the arXiv API on the given URL/\n",
    "\n",
    "    url -> str\n",
    "        The given url to send the fetch request to\n",
    "\n",
    "    params -> dict\n",
    "        Parameters of the fetch request\n",
    "\n",
    "    Returns -> feedparser.util.FeedParserDict\n",
    "        A feedparser.util.FeedParserDict object that contains the JSON parsed data\n",
    "    \n",
    "    Example\n",
    "        fetch_request(helper.DEFAULT_URL, helper.DEFAULT_PARAMS)\n",
    "        See `helper.py` file for the formats of the default values\n",
    "    \"\"\"\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        feed = feedparser.parse(response.content)\n",
    "        print(f\"Fetched {len(feed.entries)} entries.\")\n",
    "        return feed\n",
    "    else:\n",
    "        raise ConnectionError(response.status_code)\n",
    "    \n",
    "def parse_request(feed: feedparser.util.FeedParserDict, verbose:int = 0) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Covnerts the given JSON feed file into a legible dataframe (useful for .csv storage).\n",
    "\n",
    "    feed -> feedparser.util.FeedParserDict\n",
    "        The given JSON object from feedparser\n",
    "\n",
    "    verbose -> int\n",
    "        0: suppresses all missing-data-points reporting\n",
    "        1: at the end of script, summarize the total number of missing data points.\n",
    "        2: for every queried paper, error report on the missing data features; at the end of script, also summarize the total number of missing data points.\n",
    "        \n",
    "    Returns -> pd.core.frame.DataFrame\n",
    "        The JSON object converted to a dataframe\n",
    "\n",
    "    Example\n",
    "        parse_request(feed, 0)\n",
    "    \"\"\"\n",
    "    all_papers = []\n",
    "    num_missing_keys = 0\n",
    "    for paper in feed.entries:\n",
    "        paper_data = []\n",
    "        for key in helper.ARXIV_KEYS:\n",
    "            try:\n",
    "                paper_data.append(paper[key])\n",
    "            except:\n",
    "                paper_data.append(np.nan)\n",
    "                if verbose == 2:\n",
    "                    print(f\"{paper['id']} does not have {key} key\")\n",
    "                num_missing_keys += 1\n",
    "        all_papers.append(paper_data)\n",
    "    if verbose == 1 or verbose == 2:\n",
    "        print(f\"{num_missing_keys} missing keys.\")\n",
    "    df = pd.DataFrame(data=all_papers, columns=helper.MASTER_CSV_COLUMNS)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_publish_index(feed: feedparser.util.FeedParserDict, tar_date: str = helper.DEFAULT_DATE_QUERY) -> int:\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    target_date_split = tar_date.split(\"-\")\n",
    "    if len(target_date_split) != 3:\n",
    "        raise ValueError(f\"ensure that your target date format is correct: year-month-day. eg. 2010-2-31\")\n",
    "    else:\n",
    "        search_target_date = datetime(int(target_date_split[0]), int(target_date_split[1]), int(target_date_split[2]))\n",
    "\n",
    "    for paper_index in range(len(feed.entries)):\n",
    "        raw_date_split = feed.entries[paper_index][\"published\"].split(\"T\")[0].split(\"-\")\n",
    "        paper_published_date = datetime(int(raw_date_split[0]), int(raw_date_split[1]), int(raw_date_split[2]))\n",
    "        if paper_published_date >= search_target_date:\n",
    "            print(f\"Publish {paper_published_date} > Search {search_target_date}\")\n",
    "            print(f\"Found index for target date: {paper_index}!\")\n",
    "            return paper_index\n",
    "    print(\"Did not find index for target date!\")\n",
    "    return None\n",
    "\n",
    "def helper_extract_start_index(query: str = helper.DEFAULT_SEARCH_QUERY, tar_date: str = helper.DEFAULT_DATE_QUERY) -> int:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # every value in the up to 30,000 records that comes with the view\n",
    "    for i in range(0, helper.MAX_VIEW_SIZE, helper.MAX_STEP_SIZE):\n",
    "        params = {\n",
    "            \"search_query\": query,\n",
    "            \"sortBy\": 'submittedDate',\n",
    "            \"sortOrder\": 'ascending',\n",
    "            \"start\": i,\n",
    "            \"max_results\": helper.MAX_STEP_SIZE\n",
    "        }\n",
    "        feed = fetch_request(helper.DEFAULT_URL, params)\n",
    "        index = crawl_publish_index(feed, tar_date)\n",
    "        if type(index) == int:\n",
    "            return index\n",
    "        time.sleep(helper.MIN_WAIT_TIME) # must be greater than 3 seconds or else arXiv will IP ban\n",
    "    print(f\"Did not find index for target date!\")\n",
    "    return None\n",
    "\n",
    "def extract_start_index(query: str = helper.DEFAULT_SEARCH_QUERY, tar_date: str = helper.DEFAULT_DATE_QUERY) -> int:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    helper_extract_start_index(query, tar_date)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>journal</th>\n",
       "      <th>authors</th>\n",
       "      <th>doi</th>\n",
       "      <th>published</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, journal, authors, doi, published, abstract, url, tags]\n",
       "Index: []"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(columns=helper.MASTER_CSV_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_meta_papers_db():\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    # Warning menu\n",
    "    while True:\n",
    "        user_input = input(\"Are you sure you want to run this function? This wipes the ENTIRE existing arxiv_meta_data.csv database! (y/n)\")\n",
    "        if user_input == \"y\":\n",
    "            print(\"Proceeding to reset database.\")\n",
    "            break\n",
    "        elif user_input == \"n\":\n",
    "            print(\"Function canceled.\")\n",
    "            return None\n",
    "        else:\n",
    "            print(\"Wrong input. Please type 'y' or 'n'.\")\n",
    "\n",
    "    # Saving data to csv\n",
    "    df = pd.DataFrame(columns=helper.META_CSV_COLUMNS)\n",
    "    try:\n",
    "        df.to_csv(\"../data/arxiv_meta_data.csv\", index=False)\n",
    "        print(\"Saved!\")\n",
    "    except:\n",
    "        print(\"Failed to save...\")\n",
    "\n",
    "    # Return\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_papers_db(query: str = helper.DEFAULT_SEARCH_QUERY, verbose:int = 0) -> None:\n",
    "    \"\"\"\n",
    "    Resets/overwrites the \"arxiv.csv\" database with the 5 most recently published papers about the given query.\n",
    "    USE WITH CAUTION.\n",
    "\n",
    "    query -> str\n",
    "        The given query/search term\n",
    "\n",
    "    verbose -> int\n",
    "        0: suppresses all missing-data-points reporting\n",
    "        1: at the end of script, summarize the total number of missing data points.\n",
    "        2: for every queried paper, error report on the missing data features; at the end of script, also summarize the total number of missing data points.\n",
    "        \n",
    "    Returns\n",
    "        None\n",
    "\n",
    "    Example\n",
    "        fetch_initial_papers(\"radiation\", 0)\n",
    "    \"\"\"\n",
    "    # Warning menu\n",
    "    while True:\n",
    "        user_input = input(\"Are you sure you want to run this function? This wipes the ENTIRE existing arxiv.csv database! (y/n)\")\n",
    "        if user_input == \"y\":\n",
    "            print(\"Proceeding to reset database.\")\n",
    "            break\n",
    "        elif user_input == \"n\":\n",
    "            print(\"Function canceled.\")\n",
    "            return None\n",
    "        else:\n",
    "            print(\"Wrong input. Please type 'y' or 'n'.\")\n",
    "\n",
    "    # Error control\n",
    "    valid_verbose = {0, 1, 2}\n",
    "    if verbose not in valid_verbose:\n",
    "        raise ValueError(f\"verbose input must be one of the following: {valid_verbose}\")\n",
    "\n",
    "    # Fetch request\n",
    "    params = {\n",
    "        \"search_query\": query,\n",
    "        \"sortBy\": 'submittedDate', # do not change this\n",
    "        \"sortOrder\": 'ascending', # do not change this\n",
    "        \"start\": 0,\n",
    "        \"max_results\": 5\n",
    "    }\n",
    "    feed = fetch_request(helper.DEFAULT_URL, params)\n",
    "\n",
    "    # Parse request\n",
    "    df = parse_request(feed, 0)\n",
    "\n",
    "    # Saving data to csv\n",
    "    try:\n",
    "        df.to_csv(\"../data/arxiv.csv\", index=False)\n",
    "        print(\"Saved!\")\n",
    "    except:\n",
    "        print(\"Failed to save...\")\n",
    "\n",
    "    # Return\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding to reset database.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'helper' has no attribute 'META_CSV_COLUMNS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m reset_meta_papers_db()\n",
      "Cell \u001b[1;32mIn[83], line 17\u001b[0m, in \u001b[0;36mreset_meta_papers_db\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWrong input. Please type \u001b[39m\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[39m# Saving data to csv\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39mhelper\u001b[39m.\u001b[39mMETA_CSV_COLUMNS)\n\u001b[0;32m     18\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     df\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39m../data/arxiv_meta_data.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'helper' has no attribute 'META_CSV_COLUMNS'"
     ]
    }
   ],
   "source": [
    "reset_meta_papers_db()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_fetch_papers(query: str = DEFAULT_SEARCH_QUERY, verbose:int = 0) -> None:\n",
    "    \"\"\"\n",
    "    Resets/overwrites the \"arxiv.csv\" database with the 5 most recently published papers about the given query.\n",
    "\n",
    "    query -> str\n",
    "        The given query/search term\n",
    "\n",
    "    verbose -> int\n",
    "        0: suppresses all missing-data-points reporting\n",
    "        1: at the end of script, summarize the total number of missing data points.\n",
    "        2: for every queried paper, error report on the missing data features; at the end of script, also summarize the total number of missing data points.\n",
    "        \n",
    "    Returns\n",
    "        None\n",
    "\n",
    "    Example\n",
    "        fetch_initial_papers(\"radiation\", 0)\n",
    "    \"\"\"\n",
    "    # Error control\n",
    "    valid_verbose = {0, 1, 2}\n",
    "    if verbose not in valid_verbose:\n",
    "        raise ValueError(f\"verbose input must be one of the following: {valid_verbose}\")\n",
    "\n",
    "    # Set request params\n",
    "    params = {\n",
    "        \"search_query\": query,\n",
    "        \"sortBy\": 'submittedDate', # do not change this\n",
    "        \"sortOrder\": 'ascending', # do not change this\n",
    "        \"start\": 0,\n",
    "        \"max_results\": 5\n",
    "    }\n",
    "\n",
    "    # Fetching request\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        feed = feedparser.parse(response.content)\n",
    "        print(f\"Fetched {len(feed.entries)} entries.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Parsing request\n",
    "    all_papers = []\n",
    "    num_missing_keys = 0\n",
    "    for paper in feed.entries:\n",
    "        paper_data = []\n",
    "        for key in ARXIV_KEYS:\n",
    "            try:\n",
    "                paper_data.append(paper[key])\n",
    "            except:\n",
    "                paper_data.append(np.nan)\n",
    "                if verbose == 2:\n",
    "                    print(f\"{paper['id']} does not have {key} key\")\n",
    "                num_missing_keys += 1\n",
    "        all_papers.append(paper_data)\n",
    "    if verbose == 1 or verbose == 2:\n",
    "        print(f\"{num_missing_keys} missing keys.\")\n",
    "\n",
    "    # Saving data to csv\n",
    "    df = pd.DataFrame(data=all_papers, columns=MASTER_CSV_COLUMNS)\n",
    "    try:\n",
    "        df.to_csv(\"../data/arxiv.csv\", index=False)\n",
    "        print(\"Saved!\")\n",
    "    except:\n",
    "        print(\"Failed to save...\")\n",
    "\n",
    "    # Return\n",
    "    return None\n",
    "\n",
    "def fetch_more_papers(query: str = DEFAULT_SEARCH_QUERY, verbose:int = 0, n:int = 50) -> None:\n",
    "    \"\"\"\n",
    "    TODO: make this docstring better lol\n",
    "    fetches the next n papers published after the oldest entry in the csv\n",
    "    \"\"\"\n",
    "    # Extract oldest published date in dataset \n",
    "    df = pd.read_csv(\"../data/arxiv.csv\")\n",
    "    start_date = df[\"published\"].iloc[-1:].values[0]\n",
    "\n",
    "    # Set request params\n",
    "    params = {\n",
    "        \"search_query\": query,\n",
    "        \"start_date\": start_date,\n",
    "        \"sortBy\": 'submittedDate',  # relevance, lastUpdatedDate, submittedDate\n",
    "        \"max_results\": n,\n",
    "        \"sortOrder\": 'descending'\n",
    "    }\n",
    "\n",
    "    # Fetching request\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        feed = feedparser.parse(response.content)\n",
    "        print(f\"Fetched {len(feed.entries)} entries.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Parsing request\n",
    "    all_papers = []\n",
    "    num_missing_keys = 0\n",
    "    for paper in feed.entries:\n",
    "        paper_data = []\n",
    "        for key in ARXIV_KEYS:\n",
    "            try:\n",
    "                paper_data.append(paper[key])\n",
    "            except:\n",
    "                paper_data.append(np.nan)\n",
    "                if verbose == 1:\n",
    "                    print(f\"{paper['id']} does not have {key} key\")\n",
    "                    num_missing_keys += 1\n",
    "                else:\n",
    "                    num_missing_keys += 1\n",
    "        all_papers.append(paper_data)\n",
    "    print(f\"{num_missing_keys} missing keys.\")\n",
    "\n",
    "    # Saving data to csv\n",
    "    df = pd.DataFrame(data=all_papers, columns=MASTER_CSV_COLUMNS)\n",
    "    try:\n",
    "        df.to_csv(\"../data/arxiv.csv\", mode=\"a\", index=False, header=False)\n",
    "        print(\"Saved!\")\n",
    "    except:\n",
    "        print(\"Failed to save...\")\n",
    "\n",
    "    # Return\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 50 entries.\n",
      "93 missing keys.\n",
      "Failed to save...\n"
     ]
    }
   ],
   "source": [
    "fetch_more_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_papers(query: str = DEFAULT_SEARCH_QUERY, verbose:int = 0, n:int = 50) -> None:\n",
    "    \"\"\" \n",
    "    TODO docstring\n",
    "\n",
    "    wrapper for fetch more papers\n",
    "    \"\"\"\n",
    "    if n <= 0:\n",
    "        print(\"n cannot be negative or 0.\")\n",
    "        return None\n",
    "    else:\n",
    "        mult_of_10 = n // 10\n",
    "        leftoever_of_10 = n - mult_of_10\n",
    "\n",
    "        if mult_of_10 > 0:\n",
    "            for _ in range(mult_of_10):\n",
    "                fetch_more_papers(query, verbose)\n",
    "        fetch_more_papers(query, verbose, leftoever_of_10)\n",
    "        print(\"Finished loops!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 10 entries.\n",
      "16 missing keys.\n",
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "# WARNING, RUNNING THIS FUNCTION WILL RESET THE DATABASE\n",
    "fetch_initial_papers(DEFAULT_SEARCH_QUERY, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1000 entries.\n",
      "1698 missing keys.\n",
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "fetch_more_papers(DEFAULT_SEARCH_QUERY, 0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DoD-XForce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
