{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the pipeline for querying and storing arxiv data into csv files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Usage / Strategy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future developers, important features about the arXiv API:\n",
    "- **it lacks a \"search by published date\" feature** (very problematic)\n",
    "- **in a query, the first return entry is always index 0** (meaning index 0 is different every query, also problematic)\n",
    "- it has the `\"start\"` & `\"max_results\"` paramaters that allow you to slice out a subsection of the entire query (starting from index `start`, return `max_result` entries)\n",
    "- it returns a max view of 30,000 but only allows you to retrieve a slice of 2000\n",
    "- it requires a 3 second wait between each query (or IP banned)\n",
    "\n",
    "I figured out a way to circumvent the lack of a \"search by published date\" feature below:\n",
    "- using the `\"sortOrder\": 'ascending'` parameter, we can **fix** the oldest entry as index 0\n",
    "- using `\"start\"` & `\"max_results\"` parameter, we can find out at which index the year 2020 papers started and then always consistently query from that point onwards\n",
    "- because there aren't 30,000 published papers in the queries this data pipeline was built upon, it is unclear how the indexing will change when the 30,000 limit has been surpassed\n",
    "    - this is a likely place to check if future bugs pop up\n",
    "\n",
    "Takeaways\n",
    "- this entire data pipeline is built on the `\"sortOrder\": 'ascending'`; please do not mess with this\n",
    "- (also, API documentation is lacking and debugging this seemingly super tiny issue took waaaay too long)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import feedparser\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables and Script Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_conn(conn)\n\u001b[0;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\urllib3\\connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39mconnect()\n\u001b[0;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\urllib3\\connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    417\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[1;32m--> 419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[0;32m    420\u001b[0m     sock\u001b[39m=\u001b[39mconn,\n\u001b[0;32m    421\u001b[0m     keyfile\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_file,\n\u001b[0;32m    422\u001b[0m     certfile\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcert_file,\n\u001b[0;32m    423\u001b[0m     key_password\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_password,\n\u001b[0;32m    424\u001b[0m     ca_certs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_certs,\n\u001b[0;32m    425\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_cert_dir,\n\u001b[0;32m    426\u001b[0m     ca_cert_data\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_cert_data,\n\u001b[0;32m    427\u001b[0m     server_hostname\u001b[39m=\u001b[39mserver_hostname,\n\u001b[0;32m    428\u001b[0m     ssl_context\u001b[39m=\u001b[39mcontext,\n\u001b[0;32m    429\u001b[0m     tls_in_tls\u001b[39m=\u001b[39mtls_in_tls,\n\u001b[0;32m    430\u001b[0m )\n\u001b[0;32m    432\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\urllib3\\util\\ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[1;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[0;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39mserver_hostname\n\u001b[0;32m    451\u001b[0m     )\n\u001b[0;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\urllib3\\util\\ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[1;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39mserver_hostname)\n\u001b[0;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msslsocket_class\u001b[39m.\u001b[39m_create(\n\u001b[0;32m    518\u001b[0m         sock\u001b[39m=\u001b[39msock,\n\u001b[0;32m    519\u001b[0m         server_side\u001b[39m=\u001b[39mserver_side,\n\u001b[0;32m    520\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39mdo_handshake_on_connect,\n\u001b[0;32m    521\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39msuppress_ragged_eofs,\n\u001b[0;32m    522\u001b[0m         server_hostname\u001b[39m=\u001b[39mserver_hostname,\n\u001b[0;32m    523\u001b[0m         context\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[0;32m    524\u001b[0m         session\u001b[39m=\u001b[39msession\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\ssl.py:1075\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1074\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1075\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_handshake()\n\u001b[0;32m   1076\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\ssl.py:1346\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 1346\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mdo_handshake()\n\u001b[0;32m   1347\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\requests\\adapters.py:487\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 487\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(\n\u001b[0;32m    488\u001b[0m         method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[0;32m    489\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[0;32m    490\u001b[0m         body\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mbody,\n\u001b[0;32m    491\u001b[0m         headers\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mheaders,\n\u001b[0;32m    492\u001b[0m         redirect\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         assert_same_host\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         preload_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         decode_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    496\u001b[0m         retries\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_retries,\n\u001b[0;32m    497\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[0;32m    498\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    499\u001b[0m     )\n\u001b[0;32m    501\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[1;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39mincrement(\n\u001b[0;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39me, _pool\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, _stacktrace\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m]\n\u001b[0;32m    789\u001b[0m )\n\u001b[0;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\urllib3\\util\\retry.py:550\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[39mif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 550\u001b[0m     \u001b[39mraise\u001b[39;00m six\u001b[39m.\u001b[39mreraise(\u001b[39mtype\u001b[39m(error), error, _stacktrace)\n\u001b[0;32m    551\u001b[0m \u001b[39melif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\urllib3\\packages\\six.py:769\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[1;32m--> 769\u001b[0m     \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m    770\u001b[0m \u001b[39mraise\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_conn(conn)\n\u001b[0;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\urllib3\\connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39mconnect()\n\u001b[0;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\urllib3\\connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    417\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[1;32m--> 419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[0;32m    420\u001b[0m     sock\u001b[39m=\u001b[39mconn,\n\u001b[0;32m    421\u001b[0m     keyfile\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_file,\n\u001b[0;32m    422\u001b[0m     certfile\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcert_file,\n\u001b[0;32m    423\u001b[0m     key_password\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_password,\n\u001b[0;32m    424\u001b[0m     ca_certs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_certs,\n\u001b[0;32m    425\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_cert_dir,\n\u001b[0;32m    426\u001b[0m     ca_cert_data\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_cert_data,\n\u001b[0;32m    427\u001b[0m     server_hostname\u001b[39m=\u001b[39mserver_hostname,\n\u001b[0;32m    428\u001b[0m     ssl_context\u001b[39m=\u001b[39mcontext,\n\u001b[0;32m    429\u001b[0m     tls_in_tls\u001b[39m=\u001b[39mtls_in_tls,\n\u001b[0;32m    430\u001b[0m )\n\u001b[0;32m    432\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\urllib3\\util\\ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[1;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[0;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39mserver_hostname\n\u001b[0;32m    451\u001b[0m     )\n\u001b[0;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\urllib3\\util\\ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[1;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39mserver_hostname)\n\u001b[0;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msslsocket_class\u001b[39m.\u001b[39m_create(\n\u001b[0;32m    518\u001b[0m         sock\u001b[39m=\u001b[39msock,\n\u001b[0;32m    519\u001b[0m         server_side\u001b[39m=\u001b[39mserver_side,\n\u001b[0;32m    520\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39mdo_handshake_on_connect,\n\u001b[0;32m    521\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39msuppress_ragged_eofs,\n\u001b[0;32m    522\u001b[0m         server_hostname\u001b[39m=\u001b[39mserver_hostname,\n\u001b[0;32m    523\u001b[0m         context\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[0;32m    524\u001b[0m         session\u001b[39m=\u001b[39msession\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\ssl.py:1075\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1074\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1075\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_handshake()\n\u001b[0;32m   1076\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\ssl.py:1346\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 1346\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mdo_handshake()\n\u001b[0;32m   1347\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m feed \u001b[39m=\u001b[39m fetch_request(start\u001b[39m=\u001b[39m\u001b[39m15758\u001b[39m, max_results\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m df \u001b[39m=\u001b[39m parse_request(feed, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m df\n",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m, in \u001b[0;36mfetch_request\u001b[1;34m(url, query, start, max_results)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mPerforms a fetch request using the arXiv API on the given URL.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39m    Query \"radiation\" returns 30,000 (default API behavior). We take the 11th article on the list (0th index) up to 49th article on the list, returning 50 total articles.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     24\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[0;32m     25\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msearch_query\u001b[39m\u001b[39m\"\u001b[39m: query,\n\u001b[0;32m     26\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msortBy\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msubmittedDate\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmax_results\u001b[39m\u001b[39m\"\u001b[39m: max_results\n\u001b[0;32m     30\u001b[0m }\n\u001b[1;32m---> 31\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url, params\u001b[39m=\u001b[39mparams)\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m     33\u001b[0m     feed \u001b[39m=\u001b[39m feedparser\u001b[39m.\u001b[39mparse(response\u001b[39m.\u001b[39mcontent)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\DoD-XForce\\Lib\\site-packages\\requests\\adapters.py:502\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    487\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(\n\u001b[0;32m    488\u001b[0m         method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[0;32m    489\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    498\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    499\u001b[0m     )\n\u001b[0;32m    501\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 502\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    504\u001b[0m \u001b[39mexcept\u001b[39;00m MaxRetryError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    505\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    506\u001b[0m         \u001b[39m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_request(url: str=helper.DEFAULT_URL, query: str=helper.DEFAULT_SEARCH_QUERY, start: int=0, max_results: int=10) -> feedparser.util.FeedParserDict:\n",
    "    \"\"\"\n",
    "    Performs a fetch request using the arXiv API on the given URL.\n",
    "\n",
    "    url -> str\n",
    "        The given url to send the fetch request to\n",
    "\n",
    "    query -> str\n",
    "        The given search query for arxiv to find papers on\n",
    "\n",
    "    start -> int\n",
    "        The index of the papers at which to start pulling data on\n",
    "\n",
    "    max_results -> int\n",
    "        The total number of papers after `start` to pull from; cannot exceed 2000\n",
    "\n",
    "    Returns -> feedparser.util.FeedParserDict\n",
    "        A feedparser.util.FeedParserDict object that contains the JSON parsed data\n",
    "    \n",
    "    Example\n",
    "        fetch_request(helper.DEFAULT_URL, \"radiation\", 10, 50)\n",
    "        Query \"radiation\" returns 30,000 (default API behavior). We take the 11th article on the list (0th index) up to 49th article on the list, returning 50 total articles.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"search_query\": query,\n",
    "        \"sortBy\": 'submittedDate',\n",
    "        \"sortOrder\": 'ascending',\n",
    "        \"start\": start,\n",
    "        \"max_results\": max_results\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        feed = feedparser.parse(response.content)\n",
    "        print(f\"Fetched {len(feed.entries)} entries.\")\n",
    "        return feed\n",
    "    else:\n",
    "        raise ConnectionError(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_request(feed: feedparser.util.FeedParserDict, query: str, verbose: int=0) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the given JSON feed file into a legible dataframe (useful for .csv storage).\n",
    "\n",
    "    feed -> feedparser.util.FeedParserDict\n",
    "        The given JSON object from feedparser\n",
    "\n",
    "    query -> str\n",
    "        The query term that was used to generate the feed. This is not enforced to be correct, so users need to manually double-check that this field is correct.\n",
    "        Used in .csv saving.\n",
    "\n",
    "    verbose -> int\n",
    "        0: suppresses all missing-data-points reporting\n",
    "        1: at the end of script, summarize the total number of missing data points.\n",
    "        2: for every queried paper, error report on the missing data features; at the end of script, also summarize the total number of missing data points.\n",
    "        \n",
    "    Returns -> pd.core.frame.DataFrame\n",
    "        The JSON object converted to a dataframe\n",
    "\n",
    "    Example\n",
    "        parse_request(feed, \"radiation\", 0)\n",
    "    \"\"\"\n",
    "    all_papers = []\n",
    "    num_missing_keys = 0\n",
    "    for paper in feed.entries:\n",
    "        paper_data = [query, datetime.now()]\n",
    "        for key in helper.ARXIV_KEYS:\n",
    "            try:\n",
    "                if key == \"summary\":\n",
    "                    paper_data.append(paper[key].replace(\"\\n\", \" \"))\n",
    "                elif key == \"authors\":\n",
    "                    paper_data.append([item[\"name\"] for item in paper[key]])\n",
    "                else:\n",
    "                    paper_data.append(paper[key])\n",
    "            except:\n",
    "                paper_data.append(np.nan)\n",
    "                if verbose == 2:\n",
    "                    print(f\"{paper['id']} does not have {key} key\")\n",
    "                num_missing_keys += 1\n",
    "        all_papers.append(paper_data)\n",
    "    if verbose == 1 or verbose == 2:\n",
    "        print(f\"{num_missing_keys} missing keys.\")\n",
    "    df = pd.DataFrame(data=all_papers, columns=helper.MASTER_CSV_COLUMNS)\n",
    "    print(\"Parsed!\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_request_for_published(feed: feedparser.util.FeedParserDict, tar_date: str=helper.DEFAULT_DATE_QUERY) -> int:\n",
    "    \"\"\"\n",
    "    Finds the first index position of a paper whose published date is either on or after the given target date (useful for selective querying).\n",
    "\n",
    "    feed -> feedparser.util.FeedParserDict\n",
    "        The given JSON object from feedparser\n",
    "\n",
    "    tar_date -> str\n",
    "        The given target date\n",
    "        \n",
    "    Returns -> int, None\n",
    "        If found index, returns int\n",
    "        Else, return None\n",
    "\n",
    "    Example\n",
    "        crawl_request_for_published(feed, \"2010-2-31\")\n",
    "    \"\"\"\n",
    "    target_date_split = tar_date.split(\"-\")\n",
    "    if len(target_date_split) != 3:\n",
    "        raise ValueError(f\"ensure that your target date format is correct: year-month-day. eg. 2010-2-31\")\n",
    "    else:\n",
    "        search_target_date = datetime(int(target_date_split[0]), int(target_date_split[1]), int(target_date_split[2]))\n",
    "\n",
    "    for paper_index in range(len(feed.entries)):\n",
    "        raw_date_split = feed.entries[paper_index][\"published\"].split(\"T\")[0].split(\"-\")\n",
    "        paper_published_date = datetime(int(raw_date_split[0]), int(raw_date_split[1]), int(raw_date_split[2]))\n",
    "        if paper_published_date >= search_target_date:\n",
    "            print(f\"Found index for target date: {paper_index}!\")\n",
    "            return paper_index\n",
    "    print(\"Did not find index for target date!\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_parse_and_crawl(query: str=helper.DEFAULT_SEARCH_QUERY, tar_date: str=helper.DEFAULT_DATE_QUERY) -> tuple:\n",
    "    \"\"\"\n",
    "    Helper function that automatically saves all queried data to database and return the starting index.\n",
    "\n",
    "    query -> str\n",
    "        The given search query\n",
    "\n",
    "    tar_date -> str\n",
    "        The given target date\n",
    "        \n",
    "    Returns -> int, None\n",
    "        If found index, returns int\n",
    "        Else, return None\n",
    "\n",
    "    Example\n",
    "        helper_parse_and_crawl(\"radiation\", \"2010-2-31\")\n",
    "    \"\"\"\n",
    "    # every value in the up to 30,000 records that comes with the view\n",
    "    final_index = 0\n",
    "    for i in range(0, helper.MAX_VIEW_SIZE, helper.MAX_STEP_SIZE):\n",
    "        # parsing all data\n",
    "        print(\"--\")\n",
    "        print(f\"Checking index {i}-{i+helper.MAX_STEP_SIZE}\")\n",
    "        print(\"Fetching request...\")\n",
    "        feed = fetch_request(helper.DEFAULT_URL, query, i, helper.MAX_STEP_SIZE)\n",
    "        print(\"Parsing request...\")\n",
    "        df = parse_request(feed, query, 0)\n",
    "        print(\"Crawling request for target date...\")\n",
    "        index = crawl_request_for_published(feed, tar_date)\n",
    "\n",
    "        # Saving data to csv\n",
    "        print(f\"Saving {i}-{i+helper.MAX_STEP_SIZE}...\")\n",
    "        try:\n",
    "            df.to_csv(\"../data/arxiv.csv\", mode='a', header=False, index=False)\n",
    "            print(\"Successfully saved!\")\n",
    "        except:\n",
    "            print(\"Failed to save.\")\n",
    "\n",
    "        # Returning index if found\n",
    "        if type(index) == int:\n",
    "            print(f\"Found target date! Sub-index: {index}, total: {final_index + index}\")\n",
    "            entry = df.iloc[index, :]\n",
    "            final_index += index\n",
    "            return (final_index, entry)\n",
    "        else:\n",
    "            final_index += helper.MAX_STEP_SIZE\n",
    "        \n",
    "        # Query wait\n",
    "        time.sleep(helper.MIN_WAIT_TIME) # wait or else arXiv will IP ban\n",
    "    print(f\"Did not find index for target date in entire window.\")\n",
    "    return (np.nan, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_papers_and_start_index(query: str=helper.DEFAULT_SEARCH_QUERY, tar_date: str=helper.DEFAULT_DATE_QUERY) -> None:\n",
    "    \"\"\"\n",
    "    Fetches papers of the query up to the tar_date; saves data in the meta-data database.\n",
    "\n",
    "    query -> str\n",
    "        The given search query \n",
    "\n",
    "    tar_date -> str\n",
    "        The given target date\n",
    "        \n",
    "    Returns\n",
    "        None\n",
    "\n",
    "    Example\n",
    "        get_papers_and_start_index(\"radiation\", \"2010-2-31\")\n",
    "    \"\"\"\n",
    "    final_index, entry = helper_parse_and_crawl(query, tar_date)\n",
    "    try:\n",
    "        data_constr = [query,\n",
    "                       entry[\"url\"],\n",
    "                       entry[\"title\"],\n",
    "                       entry[\"published\"],\n",
    "                       final_index,\n",
    "                       tar_date,\n",
    "                       datetime.now()\n",
    "                       ]\n",
    "    except:\n",
    "        data_constr = [query,\n",
    "                       np.nan,\n",
    "                       np.nan,\n",
    "                       np.nan,\n",
    "                       np.nan,\n",
    "                       tar_date,\n",
    "                       datetime.now()\n",
    "                       ]\n",
    "    \n",
    "    # Saving \n",
    "    df = pd.DataFrame(data=np.array(data_constr).reshape(1, 7), columns=helper.META_CSV_COLUMNS)\n",
    "    try:\n",
    "        df.to_csv(\"../data/arxiv_meta_data.csv\", mode='a', index=False, header=False)\n",
    "        print(\"Saved!\")\n",
    "    except:\n",
    "        print(\"Failed to save...\")\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_meta_papers_db():\n",
    "    \"\"\"\n",
    "    Overwrites the existing metadata database.\n",
    "    USE WITH CAUTION.\n",
    "\n",
    "    Returns\n",
    "        None\n",
    "\n",
    "    Example\n",
    "        reset_meta_papers_db()\n",
    "    \"\"\"\n",
    "    # Warning menu\n",
    "    while True:\n",
    "        user_input = input(\"Are you sure you want to run this function? This wipes the ENTIRE existing arxiv_meta_data.csv database! (y/n)\")\n",
    "        if user_input == \"y\":\n",
    "            print(\"Proceeding to reset database.\")\n",
    "            break\n",
    "        elif user_input == \"n\":\n",
    "            print(\"Function canceled.\")\n",
    "            return None\n",
    "        else:\n",
    "            print(\"Wrong input. Please type 'y' or 'n'.\")\n",
    "\n",
    "    # Saving data to csv\n",
    "    df = pd.DataFrame(columns=helper.META_CSV_COLUMNS)\n",
    "    try:\n",
    "        df.to_csv(\"../data/arxiv_meta_data.csv\", index=False)\n",
    "        print(\"Saved!\")\n",
    "    except:\n",
    "        print(\"Failed to save...\")\n",
    "\n",
    "    # Return\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_papers_db() -> None:\n",
    "    \"\"\"\n",
    "    Resets/overwrites the \"arxiv.csv\" database.\n",
    "    USE WITH CAUTION.\n",
    "\n",
    "    Returns\n",
    "        None\n",
    "\n",
    "    Example\n",
    "        reset_papers_db()\n",
    "    \"\"\"\n",
    "    # Warning menu\n",
    "    while True:\n",
    "        user_input = input(\"Are you sure you want to run this function? This wipes the ENTIRE existing arxiv.csv database! (y/n)\")\n",
    "        if user_input == \"y\":\n",
    "            print(\"Proceeding to reset database.\")\n",
    "            break\n",
    "        elif user_input == \"n\":\n",
    "            print(\"Function canceled.\")\n",
    "            return None\n",
    "        else:\n",
    "            print(\"Wrong input. Please type 'y' or 'n'.\")\n",
    "\n",
    "    # Saving data to csv\n",
    "    df = pd.DataFrame(columns=helper.MASTER_CSV_COLUMNS)\n",
    "    try:\n",
    "        df.to_csv(\"../data/arxiv.csv\", index=False)\n",
    "        print(\"Saved!\")\n",
    "    except:\n",
    "        print(\"Failed to save...\")\n",
    "\n",
    "    # Return\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESET ARXIV DATABASES; CAREFUL WHEN RUNNING\n",
    "# reset_meta_papers_db#()\n",
    "# reset_papers_db#()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding to reset database.\n",
      "Saved!\n",
      "Proceeding to reset database.\n",
      "Saved!\n",
      "Checking index 0-2000\n",
      "Fetching request...\n",
      "Fetched 2000 entries.\n",
      "Parsing request...\n",
      "Parsed!\n",
      "Crawling request for target date...\n",
      "Did not find index for target date!\n",
      "Saving 0-2000...\n",
      "Successfully saved!\n",
      "Checking index 2000-4000\n",
      "Fetching request...\n",
      "Fetched 2000 entries.\n",
      "Parsing request...\n",
      "Parsed!\n",
      "Crawling request for target date...\n",
      "Did not find index for target date!\n",
      "Saving 2000-4000...\n",
      "Successfully saved!\n",
      "Checking index 4000-6000\n",
      "Fetching request...\n",
      "Fetched 2000 entries.\n",
      "Parsing request...\n",
      "Parsed!\n",
      "Crawling request for target date...\n",
      "Did not find index for target date!\n",
      "Saving 4000-6000...\n",
      "Successfully saved!\n",
      "Checking index 6000-8000\n",
      "Fetching request...\n",
      "Fetched 2000 entries.\n",
      "Parsing request...\n",
      "Parsed!\n",
      "Crawling request for target date...\n",
      "Did not find index for target date!\n",
      "Saving 6000-8000...\n",
      "Successfully saved!\n",
      "Checking index 8000-10000\n",
      "Fetching request...\n",
      "Fetched 2000 entries.\n",
      "Parsing request...\n",
      "Parsed!\n",
      "Crawling request for target date...\n",
      "Did not find index for target date!\n",
      "Saving 8000-10000...\n",
      "Successfully saved!\n",
      "Checking index 10000-12000\n",
      "Fetching request...\n",
      "Fetched 2000 entries.\n",
      "Parsing request...\n",
      "Parsed!\n",
      "Crawling request for target date...\n",
      "Did not find index for target date!\n",
      "Saving 10000-12000...\n",
      "Successfully saved!\n",
      "Checking index 12000-14000\n",
      "Fetching request...\n",
      "Fetched 0 entries.\n",
      "Parsing request...\n",
      "Parsed!\n",
      "Crawling request for target date...\n",
      "Did not find index for target date!\n",
      "Saving 12000-14000...\n",
      "Successfully saved!\n",
      "Checking index 14000-16000\n",
      "Fetching request...\n",
      "Fetched 2000 entries.\n",
      "Parsing request...\n",
      "Parsed!\n",
      "Crawling request for target date...\n",
      "Did not find index for target date!\n",
      "Saving 14000-16000...\n",
      "Successfully saved!\n",
      "Checking index 16000-18000\n",
      "Fetching request...\n",
      "Fetched 2000 entries.\n",
      "Parsing request...\n",
      "Parsed!\n",
      "Crawling request for target date...\n",
      "Found index for target date: 1758!\n",
      "Saving 16000-18000...\n",
      "Successfully saved!\n",
      "Found target date! Sub-index: 1758, total: 17758\n",
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "get_papers_and_start_index(\"radiation\", helper.DEFAULT_DATE_QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "15758"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNDER Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_fetch_papers(query: str=helper.DEFAULT_SEARCH_QUERY, verbose:int=0) -> None:\n",
    "    \"\"\"\n",
    "    Resets/overwrites the \"arxiv.csv\" database with the 5 most recently published papers about the given query.\n",
    "\n",
    "    query -> str\n",
    "        The given query/search term\n",
    "\n",
    "    verbose -> int\n",
    "        0: suppresses all missing-data-points reporting\n",
    "        1: at the end of script, summarize the total number of missing data points.\n",
    "        2: for every queried paper, error report on the missing data features; at the end of script, also summarize the total number of missing data points.\n",
    "        \n",
    "    Returns\n",
    "        None\n",
    "\n",
    "    Example\n",
    "        fetch_initial_papers(\"radiation\", 0)\n",
    "    \"\"\"\n",
    "    # Error control\n",
    "    valid_verbose = {0, 1, 2}\n",
    "    if verbose not in valid_verbose:\n",
    "        raise ValueError(f\"verbose input must be one of the following: {valid_verbose}\")\n",
    "\n",
    "    # Set request params\n",
    "    params = {\n",
    "        \"search_query\": query,\n",
    "        \"sortBy\": 'submittedDate', # do not change this\n",
    "        \"sortOrder\": 'ascending', # do not change this\n",
    "        \"start\": 0,\n",
    "        \"max_results\": 5\n",
    "    }\n",
    "\n",
    "    # Fetching request\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        feed = feedparser.parse(response.content)\n",
    "        print(f\"Fetched {len(feed.entries)} entries.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Parsing request\n",
    "    all_papers = []\n",
    "    num_missing_keys = 0\n",
    "    for paper in feed.entries:\n",
    "        paper_data = []\n",
    "        for key in ARXIV_KEYS:\n",
    "            try:\n",
    "                paper_data.append(paper[key])\n",
    "            except:\n",
    "                paper_data.append(np.nan)\n",
    "                if verbose == 2:\n",
    "                    print(f\"{paper['id']} does not have {key} key\")\n",
    "                num_missing_keys += 1\n",
    "        all_papers.append(paper_data)\n",
    "    if verbose == 1 or verbose == 2:\n",
    "        print(f\"{num_missing_keys} missing keys.\")\n",
    "\n",
    "    # Saving data to csv\n",
    "    df = pd.DataFrame(data=all_papers, columns=helper.MASTER_CSV_COLUMNS)\n",
    "    try:\n",
    "        df.to_csv(\"../data/arxiv.csv\", index=False)\n",
    "        print(\"Saved!\")\n",
    "    except:\n",
    "        print(\"Failed to save...\")\n",
    "\n",
    "    # Return\n",
    "    return None\n",
    "\n",
    "def fetch_more_papers(query: str=helper.DEFAULT_SEARCH_QUERY, verbose: int=0, n: int=50) -> None:\n",
    "    \"\"\"\n",
    "    TODO: make this docstring better lol\n",
    "    fetches the next n papers published after the oldest entry in the csv\n",
    "    \"\"\"\n",
    "    # Extract oldest published date in dataset \n",
    "    df = pd.read_csv(\"../data/arxiv.csv\")\n",
    "    start_date = df[\"published\"].iloc[-1:].values[0]\n",
    "\n",
    "    # Set request params\n",
    "    params = {\n",
    "        \"search_query\": query,\n",
    "        \"start_date\": start_date,\n",
    "        \"sortBy\": 'submittedDate',  # relevance, lastUpdatedDate, submittedDate\n",
    "        \"max_results\": n,\n",
    "        \"sortOrder\": 'descending'\n",
    "    }\n",
    "\n",
    "    # Fetching request\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        feed = feedparser.parse(response.content)\n",
    "        print(f\"Fetched {len(feed.entries)} entries.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Parsing request\n",
    "    all_papers = []\n",
    "    num_missing_keys = 0\n",
    "    for paper in feed.entries:\n",
    "        paper_data = []\n",
    "        for key in ARXIV_KEYS:\n",
    "            try:\n",
    "                paper_data.append(paper[key])\n",
    "            except:\n",
    "                paper_data.append(np.nan)\n",
    "                if verbose == 1:\n",
    "                    print(f\"{paper['id']} does not have {key} key\")\n",
    "                    num_missing_keys += 1\n",
    "                else:\n",
    "                    num_missing_keys += 1\n",
    "        all_papers.append(paper_data)\n",
    "    print(f\"{num_missing_keys} missing keys.\")\n",
    "\n",
    "    # Saving data to csv\n",
    "    df = pd.DataFrame(data=all_papers, columns=MASTER_CSV_COLUMNS)\n",
    "    try:\n",
    "        df.to_csv(\"../data/arxiv.csv\", mode=\"a\", index=False, header=False)\n",
    "        print(\"Saved!\")\n",
    "    except:\n",
    "        print(\"Failed to save...\")\n",
    "\n",
    "    # Return\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fetch_more_papers()\n",
      "Cell \u001b[1;32mIn[12], line 89\u001b[0m, in \u001b[0;36mfetch_more_papers\u001b[1;34m(query, verbose, n)\u001b[0m\n\u001b[0;32m     80\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[0;32m     81\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msearch_query\u001b[39m\u001b[39m\"\u001b[39m: query,\n\u001b[0;32m     82\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstart_date\u001b[39m\u001b[39m\"\u001b[39m: start_date,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msortOrder\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mdescending\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     86\u001b[0m }\n\u001b[0;32m     88\u001b[0m \u001b[39m# Fetching request\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url, params\u001b[39m=\u001b[39mparams)\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m     91\u001b[0m     feed \u001b[39m=\u001b[39m feedparser\u001b[39m.\u001b[39mparse(response\u001b[39m.\u001b[39mcontent)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'url' is not defined"
     ]
    }
   ],
   "source": [
    "fetch_more_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_papers(query: str = DEFAULT_SEARCH_QUERY, verbose:int = 0, n:int = 50) -> None:\n",
    "    \"\"\" \n",
    "    TODO docstring\n",
    "\n",
    "    wrapper for fetch more papers\n",
    "    \"\"\"\n",
    "    if n <= 0:\n",
    "        print(\"n cannot be negative or 0.\")\n",
    "        return None\n",
    "    else:\n",
    "        mult_of_10 = n // 10\n",
    "        leftoever_of_10 = n - mult_of_10\n",
    "\n",
    "        if mult_of_10 > 0:\n",
    "            for _ in range(mult_of_10):\n",
    "                fetch_more_papers(query, verbose)\n",
    "        fetch_more_papers(query, verbose, leftoever_of_10)\n",
    "        print(\"Finished loops!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 10 entries.\n",
      "16 missing keys.\n",
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "# WARNING, RUNNING THIS FUNCTION WILL RESET THE DATABASE\n",
    "fetch_initial_papers(DEFAULT_SEARCH_QUERY, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1000 entries.\n",
      "1698 missing keys.\n",
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "fetch_more_papers(DEFAULT_SEARCH_QUERY, 0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DoD-XForce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
