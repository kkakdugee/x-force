{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the pipeline for querying and storing arxiv data into csv files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import feedparser\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables and Script Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import MASTER_CSV_COLUMNS, DEFAULT_SEARCH_QUERY, ARXIV_KEYS\n",
    "\n",
    "base_url = \"https://export.arxiv.org/api/\"\n",
    "endpoint = \"query\"\n",
    "url = base_url + endpoint "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_initial_papers(query: str = DEFAULT_SEARCH_QUERY, verbose:int = 0) -> None:\n",
    "    \"\"\"\n",
    "    TODO: make this docstring better lol\n",
    "    most recent fetches 10 papers\n",
    "    \"\"\"\n",
    "    # Set request params\n",
    "    params = {\n",
    "        \"search_query\": query,\n",
    "        \"sortBy\": 'submittedDate',\n",
    "        \"sortOrder\": 'descending'\n",
    "    }\n",
    "\n",
    "    # Fetching request\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        feed = feedparser.parse(response.content)\n",
    "        print(f\"Fetched {len(feed.entries)} entries.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    # Parsing request\n",
    "    all_papers = []\n",
    "    num_missing_keys = 0\n",
    "    for paper in feed.entries:\n",
    "        paper_data = []\n",
    "        for key in ARXIV_KEYS:\n",
    "            try:\n",
    "                paper_data.append(paper[key])\n",
    "            except:\n",
    "                paper_data.append(np.nan)\n",
    "                if verbose == 1:\n",
    "                    print(f\"{paper['id']} does not have {key} key\")\n",
    "                    num_missing_keys += 1\n",
    "                else:\n",
    "                    num_missing_keys += 1\n",
    "        all_papers.append(paper_data)\n",
    "    print(f\"{num_missing_keys} missing keys.\")\n",
    "\n",
    "    # Saving data to csv\n",
    "    df = pd.DataFrame(data=all_papers, columns=MASTER_CSV_COLUMNS)\n",
    "    try:\n",
    "        df.to_csv(\"../data/arxiv.csv\", index=False)\n",
    "        print(\"Saved!\")\n",
    "    except:\n",
    "        print(\"Failed to save...\")\n",
    "\n",
    "    # Return\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_more_papers(query: str = DEFAULT_SEARCH_QUERY, verbose:int = 0, n:int = 50) -> None:\n",
    "    \"\"\"\n",
    "    TODO: make this docstring better lol\n",
    "    fetches the next n papers published after the oldest entry in the csv\n",
    "    \"\"\"\n",
    "    # Extract oldest published date in dataset \n",
    "    df = pd.read_csv(\"../data/arxiv.csv\")\n",
    "    start_date = df[\"published\"].iloc[-1:].values[0]\n",
    "\n",
    "    # Set request params\n",
    "    params = {\n",
    "        \"search_query\": query,\n",
    "        \"start_date\": start_date,\n",
    "        \"sortBy\": 'submittedDate',  # relevance, lastUpdatedDate, submittedDate\n",
    "        \"max_results\": n,\n",
    "        \"sortOrder\": 'descending'\n",
    "    }\n",
    "\n",
    "    # Fetching request\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        feed = feedparser.parse(response.content)\n",
    "        print(f\"Fetched {len(feed.entries)} entries.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    # Parsing request\n",
    "    all_papers = []\n",
    "    num_missing_keys = 0\n",
    "    for paper in feed.entries:\n",
    "        paper_data = []\n",
    "        for key in ARXIV_KEYS:\n",
    "            try:\n",
    "                paper_data.append(paper[key])\n",
    "            except:\n",
    "                paper_data.append(np.nan)\n",
    "                if verbose == 1:\n",
    "                    print(f\"{paper['id']} does not have {key} key\")\n",
    "                    num_missing_keys += 1\n",
    "                else:\n",
    "                    num_missing_keys += 1\n",
    "        all_papers.append(paper_data)\n",
    "    print(f\"{num_missing_keys} missing keys.\")\n",
    "\n",
    "    # Saving data to csv\n",
    "    df = pd.DataFrame(data=all_papers, columns=MASTER_CSV_COLUMNS)\n",
    "    try:\n",
    "        df.to_csv(\"../data/arxiv.csv\", mode=\"a\", index=False, header=False)\n",
    "        print(\"Saved!\")\n",
    "    except:\n",
    "        print(\"Failed to save...\")\n",
    "\n",
    "    # Return\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_papers(query: str = DEFAULT_SEARCH_QUERY, verbose:int = 0, n:int = 50) -> None:\n",
    "    \"\"\" \n",
    "    TODO docstring\n",
    "\n",
    "    wrapper for fetch more papers\n",
    "    \"\"\"\n",
    "    if n <= 0:\n",
    "        print(\"n cannot be negative or 0.\")\n",
    "        return\n",
    "    else:\n",
    "        mult_of_10 = n // 10\n",
    "        leftoever_of_10 = n - mult_of_10\n",
    "\n",
    "        if mult_of_10 > 0:\n",
    "            for _ in range(mult_of_10):\n",
    "                fetch_more_papers(query, verbose)\n",
    "        fetch_more_papers(query, verbose, leftoever_of_10)\n",
    "        print(\"Finished loops!\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 10 entries.\n",
      "16 missing keys.\n",
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "# WARNING, RUNNING THIS FUNCTION WILL RESET THE DATABASE\n",
    "fetch_initial_papers(DEFAULT_SEARCH_QUERY, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1000 entries.\n",
      "1698 missing keys.\n",
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "fetch_more_papers(DEFAULT_SEARCH_QUERY, 0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DoD-XForce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
